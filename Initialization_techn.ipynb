{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Initialization_techn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEqkCCKzBH7AXYjsGR4LAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souayboss/Deep_learning/blob/main/Initialization_techn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3x8ezyNwVK-"
      },
      "source": [
        "import numpy as np\n",
        "import time \n",
        "import pickle as pk\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense , Conv2D, concatenate, MaxPooling2D, UpSampling2D, Input \n",
        "from tensorflow.keras.layers import Softmax, Dropout, Flatten,  ReLU, concatenate, BatchNormalization\n",
        "from tensorflow.keras import Model as Model_ , Sequential\n",
        "# drive.mount('/content/gdrive', force_remount=True)i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lsWgOnwbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbae31c-eca3-4dc6-cf27-5ce960aec8a9"
      },
      "source": [
        "# WRAP THESE ON A FUNCTION or CLASS CALL!\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train/255.\n",
        "x_test  = x_test / 255."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyLDovH6nl5d"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class KGene_conv_b(Model_):\n",
        "  def __init__(self, outchannel, kernel=3, stride=1, padding='valid', pool=False, Batchnorm=False, prebatch=True, init_ref=\"he_uniform\"):\n",
        "    super(KGene_conv_b, self).__init__() \n",
        "    layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref),ReLU()]\n",
        "    if pool:\n",
        "      layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "    if Batchnorm:\n",
        "      layer_block.append(BatchNormalization())\n",
        "    if prebatch:\n",
        "      layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref), BatchNormalization( ),\n",
        "                     ReLU()]\n",
        "      if pool:\n",
        "        layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "\n",
        "    self.conv = Sequential(layer_block)\n",
        "  def call(self, x):\n",
        "    conv = self.conv(x)\n",
        "    return conv\n",
        "  \n",
        "    \n",
        "class KGene_fully_block(Model_):\n",
        "  def __init__(self, outsize, dropout, init_ref):\n",
        "    super(KGene_fully_block, self).__init__()\n",
        "    self.fc = Sequential([                        \n",
        "      Dense(outsize, kernel_initializer=init_ref),\n",
        "      ReLU(),\n",
        "      Dropout(dropout),\n",
        "    ])\n",
        "  def call(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Generic32Batch(Model_):\n",
        "  def __init__(self, dropout=0.5, batchnorm=True, init_ref=\"he_uniform\"):\n",
        "    super(Generic32Batch, self).__init__()\n",
        "    self.conv1 = KGene_conv_b(32, padding='same', Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv2 = KGene_conv_b(64,kernel=3, padding='same',pool=True, Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv3 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref) \n",
        "    self.conv4 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref)\n",
        "    self.conv5 = KGene_conv_b(256,kernel=3, padding='same',init_ref=init_ref,pool=True)\n",
        "    self.flatt  = Flatten()\n",
        "    self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(2,2))\n",
        "    self.fc1   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.fc2   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.out   = Dense(10)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, input_x):\n",
        "    x = self.conv1(input_x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.flatt(x)  \n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    out = self.out(x)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "    \n",
        "\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(filters = 32, kernel_size = (3, 3),\n",
        "#           input_shape = (32, 32, 3),\n",
        "#           activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# model.add(Conv2D(32, 3, 3, activation = 'relu'))\n",
        "# model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(units = 128, activation = 'relu'))\n",
        "# model.add(Dense(units = 10, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzOPxTRrnpfG"
      },
      "source": [
        "model = Generic32Batch()\n",
        "Optim = Optimizer(model, mb=32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgKgAfaUqQGU",
        "outputId": "fad9d829-5d1b-4a16-e058-6a2222fdb296"
      },
      "source": [
        "%time trl, tsl, tra,tsa = Optim.run(x_train, y_train, x_test, y_test, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1,  TRAIN LOSS:  1.735732157681893, TEST LOSS:  1.2893610223413656,  TRAIN ACC:  38.42, TEST ACC:  54.949999999999996\n",
            "epoch:  2,  TRAIN LOSS:  1.0475260678080511, TEST LOSS:  0.9542749342255699,  TRAIN ACC:  63.754, TEST ACC:  66.25999999999999\n",
            "epoch:  3,  TRAIN LOSS:  0.7974077291400556, TEST LOSS:  0.8967943204858433,  TRAIN ACC:  72.744, TEST ACC:  69.66\n",
            "epoch:  4,  TRAIN LOSS:  0.6654484946702584, TEST LOSS:  0.753165546126259,  TRAIN ACC:  77.78399999999999, TEST ACC:  74.71\n",
            "epoch:  5,  TRAIN LOSS:  0.5535016564779837, TEST LOSS:  0.7031292591612941,  TRAIN ACC:  81.434, TEST ACC:  77.03\n",
            "epoch:  6,  TRAIN LOSS:  0.4727440824468816, TEST LOSS:  0.5687971408374775,  TRAIN ACC:  84.176, TEST ACC:  80.65\n",
            "epoch:  7,  TRAIN LOSS:  0.4124256858193409, TEST LOSS:  0.6092192958623838,  TRAIN ACC:  86.198, TEST ACC:  80.39\n",
            "epoch:  8,  TRAIN LOSS:  0.35761653313261915, TEST LOSS:  0.614938711348814,  TRAIN ACC:  88.068, TEST ACC:  79.84\n",
            "epoch:  9,  TRAIN LOSS:  0.31718587838661494, TEST LOSS:  0.7171681261005493,  TRAIN ACC:  89.24600000000001, TEST ACC:  78.46\n",
            "epoch:  10,  TRAIN LOSS:  0.2796185882053006, TEST LOSS:  0.6279095065431853,  TRAIN ACC:  90.476, TEST ACC:  81.07\n",
            "Training: complete!\n",
            "CPU times: user 2min 51s, sys: 26.3 s, total: 3min 17s\n",
            "Wall time: 11min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z32XY2qvnt9I"
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iqSz-ivntzk",
        "outputId": "3ca0ae23-dcc5-45bc-95ec-1d561f893d78"
      },
      "source": [
        "%time model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 2.2883 - accuracy: 0.3012 - val_loss: 1.2035 - val_accuracy: 0.5793\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.0700 - accuracy: 0.6264 - val_loss: 0.8947 - val_accuracy: 0.6930\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.8075 - accuracy: 0.7231 - val_loss: 0.7114 - val_accuracy: 0.7589\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.6647 - accuracy: 0.7750 - val_loss: 0.6665 - val_accuracy: 0.7712\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.5565 - accuracy: 0.8152 - val_loss: 0.6908 - val_accuracy: 0.7673\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4794 - accuracy: 0.8411 - val_loss: 0.5413 - val_accuracy: 0.8183\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.4151 - accuracy: 0.8607 - val_loss: 0.7839 - val_accuracy: 0.7480\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3570 - accuracy: 0.8800 - val_loss: 0.5950 - val_accuracy: 0.8111\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.3244 - accuracy: 0.8902 - val_loss: 0.5457 - val_accuracy: 0.8212\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2768 - accuracy: 0.9055 - val_loss: 0.5397 - val_accuracy: 0.8281\n",
            "CPU times: user 2min 1s, sys: 25.2 s, total: 2min 26s\n",
            "Wall time: 2min 14s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe0ac237208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUYTgI7E2C46"
      },
      "source": [
        " \n",
        "import tensorflow as tf\n",
        "import os\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "class Optimizer:\n",
        "  def __init__(self, model, mb = 8, lr = 0.001, loss = tf.keras.losses.SparseCategoricalCrossentropy, opt=tf.keras.optimizers.Adam):\n",
        "     \n",
        "    self.model     = model\n",
        "    self.loss      = loss()\n",
        "    self.optimizer = opt(learning_rate = lr)  # initialize the optimizer with the given lerning rate\n",
        "    self.mb        = mb\n",
        "    self.train_loss     = tf.keras.metrics.Mean(name='train_loss')  # hold the training loss\n",
        "\n",
        "    self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "    self.test_loss     = tf.keras.metrics.Mean(name='test_loss')\n",
        "    self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, x , y):\n",
        "    with tf.GradientTape() as tape:           # Record operations for automatic differentiation ()\n",
        "      predictions = self.model(x,training=True)\n",
        "      loss = self.loss(y, predictions)        # calculting the loss\n",
        "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # backpropagating\n",
        "    self.train_loss(loss)                     # recording the training loss\n",
        "    self.train_accuracy(y, predictions)\n",
        "    return loss\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, x , y):\n",
        "    predictions = self.model(x, training=False)\n",
        "    loss = self.loss(y, predictions)\n",
        "    self.test_loss(loss)                      #recording the test loss\n",
        "    self.test_accuracy(y, predictions)\n",
        "\n",
        "  def train (self):\n",
        "    for mbX, mbY in self.train_ds:\n",
        "      self.train_step(mbX, mbY)\n",
        "\n",
        "\n",
        "  def test  (self):\n",
        "    for mbX, mbY in self.test_ds:\n",
        "      self.test_step(mbX, mbY)\n",
        "  def run(self, dataX, dataY, testX, testY, epochs, verbose=2 ):\n",
        "    historyTR = []\n",
        "    historyTS = []\n",
        "    historyTRAC = []\n",
        "    historyTSAC = []\n",
        "     \n",
        "    template = '{} {}, {} {},{} {}, {} {},{} {}'\n",
        "    # shuffle the training data with a buffer size of 16000\n",
        "    self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(1000).batch(self.mb)\n",
        "    self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.mb)\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        self.train ()\n",
        "        self.test()\n",
        "        \n",
        "        if verbose > 0:\n",
        "            print(template.format(\"epoch: \", epoch+1,\n",
        "                          \" TRAIN LOSS: \", self.train_loss.result(),\n",
        "                          \" TEST LOSS: \" , self.test_loss.result(),\n",
        "                          \" TRAIN ACC: \" , self.train_accuracy.result()*100,\n",
        "                          \" TEST ACC: \"  , self.test_accuracy.result()*100)\n",
        "            )\n",
        "        temp = '{}'\n",
        "        historyTR.append(float(temp.format(self.train_loss.result())))\n",
        "        historyTS.append(float(temp.format(self.test_loss.result() )))\n",
        "        historyTRAC.append(float(temp.format(self.train_accuracy.result())))\n",
        "        historyTSAC.append(float(temp.format(self.test_accuracy.result())))\n",
        "\n",
        "        self.train_loss.reset_states()\n",
        "        self.train_accuracy.reset_states()\n",
        "        self.test_loss.reset_states()\n",
        "        self.test_accuracy.reset_states()\n",
        "    print('Training: complete!')\n",
        "    return historyTR,historyTS, historyTRAC,historyTSAC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrGOGYZ4D6f"
      },
      "source": [
        "# Using He/ kaiming initializer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYh82OQJ2ER5"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class KGene_conv_b(Model_):\n",
        "  def __init__(self, outchannel, kernel=3, stride=1, padding='valid', pool=False, Batchnorm=False, prebatch=True, init_ref=\"he_uniform\"):\n",
        "    super(KGene_conv_b, self).__init__() \n",
        "    layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref),ReLU()]\n",
        "    if pool:\n",
        "      layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "    if Batchnorm:\n",
        "      layer_block.append(BatchNormalization())\n",
        "    if prebatch:\n",
        "      layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref), BatchNormalization( ),\n",
        "                     ReLU()]\n",
        "      if pool:\n",
        "        layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "\n",
        "    self.conv = Sequential(layer_block)\n",
        "  def call(self, x):\n",
        "    conv = self.conv(x)\n",
        "    return conv\n",
        "  \n",
        "    \n",
        "class KGene_fully_block(Model_):\n",
        "  def __init__(self, outsize, dropout, init_ref):\n",
        "    super(KGene_fully_block, self).__init__()\n",
        "    self.fc = Sequential([                        \n",
        "      Dense(outsize, kernel_initializer=init_ref),\n",
        "      ReLU(),\n",
        "      Dropout(dropout),\n",
        "    ])\n",
        "  def call(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Generic32Batch(Model_):\n",
        "  def __init__(self, dropout=0.5, batchnorm=True, init_ref=\"he_uniform\"):\n",
        "    super(Generic32Batch, self).__init__()\n",
        "    self.conv1 = KGene_conv_b(32, padding='same', Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv2 = KGene_conv_b(64,kernel=3, padding='same',pool=True, Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv3 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref) \n",
        "    self.conv4 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref)\n",
        "    self.conv5 = KGene_conv_b(256,kernel=3, padding='same',init_ref=init_ref,pool=True)\n",
        "    self.flatt  = Flatten()\n",
        "    self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(2,2))\n",
        "    self.fc1   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.fc2   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.out   = Dense(10)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, input_x):\n",
        "    x = self.conv1(input_x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.flatt(x)  \n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    out = self.out(x)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAh-ow6X3vt1"
      },
      "source": [
        "gmodel32batch = Generic32Batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eo9fnoB30au",
        "outputId": "13277c10-09af-4acb-a87f-4f1b4677dfd1"
      },
      "source": [
        "gmodel32batch.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist = gmodel32batch.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.7112 - accuracy: 0.3938 - val_loss: 1.2346 - val_accuracy: 0.5626\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.0683 - accuracy: 0.6250 - val_loss: 0.9229 - val_accuracy: 0.6834\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8059 - accuracy: 0.7235 - val_loss: 0.9210 - val_accuracy: 0.7001\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6640 - accuracy: 0.7780 - val_loss: 0.8114 - val_accuracy: 0.7328\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5557 - accuracy: 0.8127 - val_loss: 0.6037 - val_accuracy: 0.8044\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4822 - accuracy: 0.8398 - val_loss: 0.5772 - val_accuracy: 0.8029\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4157 - accuracy: 0.8607 - val_loss: 0.5617 - val_accuracy: 0.8138\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3641 - accuracy: 0.8786 - val_loss: 0.6190 - val_accuracy: 0.7973\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3121 - accuracy: 0.8952 - val_loss: 0.5490 - val_accuracy: 0.8263\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2821 - accuracy: 0.9049 - val_loss: 0.5568 - val_accuracy: 0.8237\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2486 - accuracy: 0.9174 - val_loss: 0.5356 - val_accuracy: 0.8333\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2226 - accuracy: 0.9258 - val_loss: 0.5170 - val_accuracy: 0.8409\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1990 - accuracy: 0.9327 - val_loss: 0.5514 - val_accuracy: 0.8404\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1887 - accuracy: 0.9391 - val_loss: 0.5347 - val_accuracy: 0.8393\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1703 - accuracy: 0.9437 - val_loss: 0.5495 - val_accuracy: 0.8383\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1624 - accuracy: 0.9469 - val_loss: 0.6138 - val_accuracy: 0.8326\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1448 - accuracy: 0.9518 - val_loss: 0.6275 - val_accuracy: 0.8405\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1443 - accuracy: 0.9538 - val_loss: 0.6158 - val_accuracy: 0.8360\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1301 - accuracy: 0.9587 - val_loss: 0.5969 - val_accuracy: 0.8489\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1260 - accuracy: 0.9614 - val_loss: 0.6705 - val_accuracy: 0.8387\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1170 - accuracy: 0.9630 - val_loss: 0.6768 - val_accuracy: 0.8353\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1167 - accuracy: 0.9628 - val_loss: 0.6874 - val_accuracy: 0.8351\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1103 - accuracy: 0.9652 - val_loss: 0.6821 - val_accuracy: 0.8432\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1073 - accuracy: 0.9669 - val_loss: 0.7356 - val_accuracy: 0.8345\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1043 - accuracy: 0.9685 - val_loss: 0.6248 - val_accuracy: 0.8540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo1lHUSz32Sp"
      },
      "source": [
        "gmodel32batch_rand = Generic32Batch(init_ref='random_normal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A16UhEgG754v",
        "outputId": "011d17c8-02a2-49e5-fc85-72fe0a35df4d"
      },
      "source": [
        "gmodel32batch_rand.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist_norm = gmodel32batch_rand.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.8521 - accuracy: 0.3578 - val_loss: 1.4198 - val_accuracy: 0.4747\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1277 - accuracy: 0.6050 - val_loss: 1.0077 - val_accuracy: 0.6436\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8479 - accuracy: 0.7114 - val_loss: 0.7999 - val_accuracy: 0.7271\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6977 - accuracy: 0.7637 - val_loss: 0.6945 - val_accuracy: 0.7632\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5868 - accuracy: 0.8036 - val_loss: 0.6541 - val_accuracy: 0.7715\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5125 - accuracy: 0.8281 - val_loss: 0.5810 - val_accuracy: 0.8045\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4469 - accuracy: 0.8493 - val_loss: 0.5826 - val_accuracy: 0.8033\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3949 - accuracy: 0.8675 - val_loss: 0.6839 - val_accuracy: 0.7724\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3450 - accuracy: 0.8850 - val_loss: 0.5385 - val_accuracy: 0.8306\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3062 - accuracy: 0.8963 - val_loss: 0.6327 - val_accuracy: 0.7996\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2697 - accuracy: 0.9088 - val_loss: 0.5736 - val_accuracy: 0.8140\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2453 - accuracy: 0.9171 - val_loss: 0.5972 - val_accuracy: 0.8210\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2193 - accuracy: 0.9269 - val_loss: 0.5691 - val_accuracy: 0.8264\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2010 - accuracy: 0.9342 - val_loss: 0.5709 - val_accuracy: 0.8283\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1852 - accuracy: 0.9383 - val_loss: 0.6203 - val_accuracy: 0.8190\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1660 - accuracy: 0.9456 - val_loss: 0.5855 - val_accuracy: 0.8463\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1591 - accuracy: 0.9480 - val_loss: 0.7841 - val_accuracy: 0.8088\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1428 - accuracy: 0.9525 - val_loss: 0.6150 - val_accuracy: 0.8420\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1371 - accuracy: 0.9558 - val_loss: 0.6675 - val_accuracy: 0.8309\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1317 - accuracy: 0.9582 - val_loss: 0.6469 - val_accuracy: 0.8380\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1213 - accuracy: 0.9607 - val_loss: 0.6930 - val_accuracy: 0.8451\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1170 - accuracy: 0.9618 - val_loss: 0.6076 - val_accuracy: 0.8476\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1176 - accuracy: 0.9631 - val_loss: 0.6060 - val_accuracy: 0.8498\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1057 - accuracy: 0.9669 - val_loss: 0.7162 - val_accuracy: 0.8259\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1043 - accuracy: 0.9672 - val_loss: 0.6771 - val_accuracy: 0.8450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlL-2kL_A4BU"
      },
      "source": [
        "# FIXING the FC Layer before the last and Kaiming/He initializer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W6Mw5DE8304"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class KGene_conv_b(Model_):\n",
        "  def __init__(self, outchannel, kernel=3, stride=1, padding='valid', pool=False, Batchnorm=False, prebatch=True, init_ref=\"he_uniform\"):\n",
        "    super(KGene_conv_b, self).__init__() \n",
        "    layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref),ReLU()]\n",
        "    if pool:\n",
        "      layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "    if Batchnorm:\n",
        "      layer_block.append(BatchNormalization())\n",
        "    if prebatch:\n",
        "      layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref), BatchNormalization( ),\n",
        "                     ReLU()]\n",
        "      if pool:\n",
        "        layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "\n",
        "    self.conv = Sequential(layer_block)\n",
        "  def call(self, x):\n",
        "    conv = self.conv(x)\n",
        "    return conv\n",
        "  \n",
        "    \n",
        "class KGene_fully_block(Model_):\n",
        "  def __init__(self, outsize, dropout, init_ref, trainable=True):\n",
        "    super(KGene_fully_block, self).__init__()\n",
        "    self.fc = Sequential([                        \n",
        "      Dense(outsize, kernel_initializer=init_ref, trainable=trainable),\n",
        "      ReLU(),\n",
        "      Dropout(dropout),\n",
        "    ])\n",
        "  def call(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Generic32Batch_he(Model_):\n",
        "  def __init__(self, dropout=0.5, batchnorm=True, init_ref=\"he_uniform\"):\n",
        "    super(Generic32Batch_he, self).__init__()\n",
        "    self.conv1 = KGene_conv_b(32, padding='same', Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv2 = KGene_conv_b(64,kernel=3, padding='same',pool=True, Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv3 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref) \n",
        "    self.conv4 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref)\n",
        "    self.conv5 = KGene_conv_b(256,kernel=3, padding='same',init_ref=init_ref,pool=True)\n",
        "    self.flatt  = Flatten()\n",
        "    self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(2,2))\n",
        "    self.fc1   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.fc2   = KGene_fully_block(1024,dropout, init_ref=init_ref, trainable=False)\n",
        "    self.out   = Dense(10)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, input_x):\n",
        "    x = self.conv1(input_x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.flatt(x)  \n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    out = self.out(x)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvnO6bWr-qj3"
      },
      "source": [
        "gmodel32fix = Generic32Batch_he()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RausxZwq-rEY",
        "outputId": "056c2249-fb7c-48d8-e236-0f73c7062fe3"
      },
      "source": [
        "gmodel32fix.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist_genbefor = gmodel32fix.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6796 - accuracy: 0.4145 - val_loss: 1.3789 - val_accuracy: 0.5211\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0146 - accuracy: 0.6518 - val_loss: 0.8986 - val_accuracy: 0.6935\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7969 - accuracy: 0.7321 - val_loss: 1.0503 - val_accuracy: 0.6461\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6642 - accuracy: 0.7785 - val_loss: 0.8489 - val_accuracy: 0.7188\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5630 - accuracy: 0.8147 - val_loss: 0.6487 - val_accuracy: 0.7827\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4969 - accuracy: 0.8371 - val_loss: 0.6073 - val_accuracy: 0.7977\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4290 - accuracy: 0.8586 - val_loss: 0.6403 - val_accuracy: 0.7974\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3742 - accuracy: 0.8763 - val_loss: 0.5367 - val_accuracy: 0.8257\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3241 - accuracy: 0.8924 - val_loss: 0.6315 - val_accuracy: 0.8022\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2924 - accuracy: 0.9025 - val_loss: 0.5841 - val_accuracy: 0.8166\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2553 - accuracy: 0.9149 - val_loss: 0.7638 - val_accuracy: 0.7768\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2287 - accuracy: 0.9229 - val_loss: 0.5739 - val_accuracy: 0.8336\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2105 - accuracy: 0.9311 - val_loss: 0.6003 - val_accuracy: 0.8251\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1831 - accuracy: 0.9395 - val_loss: 0.5232 - val_accuracy: 0.8482\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1691 - accuracy: 0.9444 - val_loss: 0.5535 - val_accuracy: 0.8386\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1544 - accuracy: 0.9486 - val_loss: 0.5829 - val_accuracy: 0.8390\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1416 - accuracy: 0.9536 - val_loss: 0.5773 - val_accuracy: 0.8464\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1341 - accuracy: 0.9566 - val_loss: 0.6772 - val_accuracy: 0.8364\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1234 - accuracy: 0.9601 - val_loss: 0.7824 - val_accuracy: 0.8034\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1173 - accuracy: 0.9625 - val_loss: 0.6412 - val_accuracy: 0.8473\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1137 - accuracy: 0.9631 - val_loss: 0.6646 - val_accuracy: 0.8417\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0984 - accuracy: 0.9680 - val_loss: 0.6931 - val_accuracy: 0.8440\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1022 - accuracy: 0.9683 - val_loss: 0.6835 - val_accuracy: 0.8436\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0947 - accuracy: 0.9699 - val_loss: 0.7055 - val_accuracy: 0.8345\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0882 - accuracy: 0.9719 - val_loss: 0.7177 - val_accuracy: 0.8433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHQpx7s3An9y"
      },
      "source": [
        "# Fix last layer and Kaiming/he intializer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSDZfB-4-sqk"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class KGene_conv_b(Model_):\n",
        "  def __init__(self, outchannel, kernel=3, stride=1, padding='valid', pool=False, Batchnorm=False, prebatch=True, init_ref=\"he_uniform\"):\n",
        "    super(KGene_conv_b, self).__init__() \n",
        "    layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref),ReLU()]\n",
        "    if pool:\n",
        "      layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "    if Batchnorm:\n",
        "      layer_block.append(BatchNormalization())\n",
        "    if prebatch:\n",
        "      layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref), BatchNormalization( ),\n",
        "                     ReLU()]\n",
        "      if pool:\n",
        "        layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "\n",
        "    self.conv = Sequential(layer_block)\n",
        "  def call(self, x):\n",
        "    conv = self.conv(x)\n",
        "    return conv\n",
        "  \n",
        "    \n",
        "class KGene_fully_block(Model_):\n",
        "  def __init__(self, outsize, dropout, init_ref, trainable=True):\n",
        "    super(KGene_fully_block, self).__init__()\n",
        "    self.fc = Sequential([                        \n",
        "      Dense(outsize, kernel_initializer=init_ref, trainable=trainable),\n",
        "      ReLU(),\n",
        "      Dropout(dropout),\n",
        "    ])\n",
        "  def call(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Generic32Batch_hel(Model_):\n",
        "  def __init__(self, dropout=0.5, batchnorm=True, init_ref=\"he_uniform\"):\n",
        "    super(Generic32Batch_hel, self).__init__()\n",
        "    self.conv1 = KGene_conv_b(32, padding='same', Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv2 = KGene_conv_b(64,kernel=3, padding='same',pool=True, Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv3 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref) \n",
        "    self.conv4 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref)\n",
        "    self.conv5 = KGene_conv_b(256,kernel=3, padding='same',init_ref=init_ref,pool=True)\n",
        "    self.flatt  = Flatten()\n",
        "    self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(2,2))\n",
        "    self.fc1   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.fc2   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.out   = Dense(10, trainable=False)\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, input_x):\n",
        "    x = self.conv1(input_x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.flatt(x)  \n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    out = self.out(x)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Umdj3xABc1"
      },
      "source": [
        "gmodel32fix_last = Generic32Batch_hel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-TVBsgyAAE0",
        "outputId": "33eb4884-4ac3-46e3-d135-da6f18fb660c"
      },
      "source": [
        "gmodel32fix_last.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist_last = gmodel32fix_last.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.7051 - accuracy: 0.4056 - val_loss: 1.2159 - val_accuracy: 0.5783\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0024 - accuracy: 0.6542 - val_loss: 0.8411 - val_accuracy: 0.6967\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7689 - accuracy: 0.7399 - val_loss: 0.6943 - val_accuracy: 0.7741\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6346 - accuracy: 0.7863 - val_loss: 0.6529 - val_accuracy: 0.7780\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5439 - accuracy: 0.8181 - val_loss: 0.5819 - val_accuracy: 0.8052\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4624 - accuracy: 0.8451 - val_loss: 0.5701 - val_accuracy: 0.8094\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3936 - accuracy: 0.8681 - val_loss: 0.6001 - val_accuracy: 0.8073\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3398 - accuracy: 0.8861 - val_loss: 0.4880 - val_accuracy: 0.8407\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2961 - accuracy: 0.9006 - val_loss: 0.5131 - val_accuracy: 0.8334\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2570 - accuracy: 0.9134 - val_loss: 0.5423 - val_accuracy: 0.8309\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2237 - accuracy: 0.9254 - val_loss: 0.6766 - val_accuracy: 0.7991\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1981 - accuracy: 0.9337 - val_loss: 0.5363 - val_accuracy: 0.8404\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1748 - accuracy: 0.9414 - val_loss: 0.5820 - val_accuracy: 0.8337\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1573 - accuracy: 0.9463 - val_loss: 0.5972 - val_accuracy: 0.8368\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1465 - accuracy: 0.9509 - val_loss: 0.6442 - val_accuracy: 0.8213\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1284 - accuracy: 0.9566 - val_loss: 0.6168 - val_accuracy: 0.8379\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1253 - accuracy: 0.9579 - val_loss: 0.5813 - val_accuracy: 0.8351\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1142 - accuracy: 0.9624 - val_loss: 0.6570 - val_accuracy: 0.8399\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1078 - accuracy: 0.9652 - val_loss: 0.5951 - val_accuracy: 0.8454\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0940 - accuracy: 0.9696 - val_loss: 0.7707 - val_accuracy: 0.8303\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0960 - accuracy: 0.9681 - val_loss: 0.6929 - val_accuracy: 0.8454\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0920 - accuracy: 0.9688 - val_loss: 0.6433 - val_accuracy: 0.8360\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0832 - accuracy: 0.9728 - val_loss: 0.6632 - val_accuracy: 0.8537\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0798 - accuracy: 0.9741 - val_loss: 0.7496 - val_accuracy: 0.8382\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0788 - accuracy: 0.9735 - val_loss: 0.7339 - val_accuracy: 0.8406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joa_ZRwIAQ8O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-kyzVFSCFkF"
      },
      "source": [
        "# Using normal distribution in the last layer and fix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7abfv7tCNtB"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class KGene_conv_b(Model_):\n",
        "  def __init__(self, outchannel, kernel=3, stride=1, padding='valid', pool=False, Batchnorm=False, prebatch=True, init_ref=\"he_uniform\"):\n",
        "    super(KGene_conv_b, self).__init__() \n",
        "    layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref),ReLU()]\n",
        "    if pool:\n",
        "      layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "    if Batchnorm:\n",
        "      layer_block.append(BatchNormalization())\n",
        "    if prebatch:\n",
        "      layer_block = [Conv2D(outchannel, kernel_size=kernel, strides=stride, padding=padding, kernel_initializer=init_ref), BatchNormalization( ),\n",
        "                     ReLU()]\n",
        "      if pool:\n",
        "        layer_block.append(MaxPooling2D((3,3),strides=2))\n",
        "\n",
        "    self.conv = Sequential(layer_block)\n",
        "  def call(self, x):\n",
        "    conv = self.conv(x)\n",
        "    return conv\n",
        "  \n",
        "    \n",
        "class KGene_fully_block(Model_):\n",
        "  def __init__(self, outsize, dropout, init_ref, trainable=True):\n",
        "    super(KGene_fully_block, self).__init__()\n",
        "    self.fc = Sequential([                        \n",
        "      Dense(outsize, kernel_initializer=init_ref, trainable=trainable),\n",
        "      ReLU(),\n",
        "      Dropout(dropout),\n",
        "    ])\n",
        "  def call(self, x):\n",
        "    return self.fc(x)\n",
        "\n",
        "class Generic32Batch_hel_Nor(Model_):\n",
        "  def __init__(self, dropout=0.5, batchnorm=True, init_ref=\"he_uniform\"):\n",
        "    super(Generic32Batch_hel_Nor, self).__init__()\n",
        "    self.conv1 = KGene_conv_b(32, padding='same', Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv2 = KGene_conv_b(64,kernel=3, padding='same',pool=True, Batchnorm=batchnorm, init_ref=init_ref)\n",
        "    self.conv3 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref) \n",
        "    self.conv4 = KGene_conv_b(128,kernel=3, padding='same',init_ref=init_ref)\n",
        "    self.conv5 = KGene_conv_b(256,kernel=3, padding='same',init_ref=init_ref,pool=True)\n",
        "    self.flatt  = Flatten()\n",
        "    self.avg_pool = tf.keras.layers.AveragePooling2D(pool_size=(2,2))\n",
        "    self.fc1   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.fc2   = KGene_fully_block(1024,dropout, init_ref=init_ref)\n",
        "    self.out   = Dense(10, trainable=False, kernel_initializer='random_normal')\n",
        "    self.softmax = Softmax()\n",
        "\n",
        "  def call(self, input_x):\n",
        "    x = self.conv1(input_x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.conv5(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.flatt(x)  \n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    \n",
        "    out = self.out(x)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB6hmkgtCVvN"
      },
      "source": [
        "gmodel32fix_lastn = Generic32Batch_hel_Nor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YRxv9JwCk6k",
        "outputId": "06967511-e74a-466b-be3d-7376fd18d2e3"
      },
      "source": [
        "gmodel32fix_lastn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist_lastn = gmodel32fix_lastn.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.8495 - accuracy: 0.3453 - val_loss: 1.2358 - val_accuracy: 0.5549\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.1230 - accuracy: 0.6076 - val_loss: 1.2934 - val_accuracy: 0.5662\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8281 - accuracy: 0.7203 - val_loss: 0.8158 - val_accuracy: 0.7241\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6668 - accuracy: 0.7743 - val_loss: 0.6496 - val_accuracy: 0.7820\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5595 - accuracy: 0.8127 - val_loss: 0.6316 - val_accuracy: 0.7904\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4733 - accuracy: 0.8423 - val_loss: 0.5558 - val_accuracy: 0.8142\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4106 - accuracy: 0.8632 - val_loss: 0.5992 - val_accuracy: 0.7996\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3542 - accuracy: 0.8814 - val_loss: 0.6117 - val_accuracy: 0.8009\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3005 - accuracy: 0.8994 - val_loss: 0.4966 - val_accuracy: 0.8441\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2660 - accuracy: 0.9107 - val_loss: 0.5349 - val_accuracy: 0.8256\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2326 - accuracy: 0.9211 - val_loss: 0.6304 - val_accuracy: 0.8073\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2016 - accuracy: 0.9321 - val_loss: 0.5896 - val_accuracy: 0.8286\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1792 - accuracy: 0.9382 - val_loss: 0.5751 - val_accuracy: 0.8363\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1627 - accuracy: 0.9456 - val_loss: 0.5946 - val_accuracy: 0.8418\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1459 - accuracy: 0.9511 - val_loss: 0.6042 - val_accuracy: 0.8344\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1348 - accuracy: 0.9555 - val_loss: 0.6175 - val_accuracy: 0.8459\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1278 - accuracy: 0.9583 - val_loss: 0.5654 - val_accuracy: 0.8507\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1149 - accuracy: 0.9626 - val_loss: 0.6378 - val_accuracy: 0.8394\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1067 - accuracy: 0.9650 - val_loss: 0.6825 - val_accuracy: 0.8389\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1014 - accuracy: 0.9672 - val_loss: 0.6868 - val_accuracy: 0.8307\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0953 - accuracy: 0.9683 - val_loss: 0.6473 - val_accuracy: 0.8442\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0938 - accuracy: 0.9690 - val_loss: 0.6867 - val_accuracy: 0.8272\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0914 - accuracy: 0.9703 - val_loss: 0.6265 - val_accuracy: 0.8502\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0838 - accuracy: 0.9729 - val_loss: 0.6647 - val_accuracy: 0.8544\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0794 - accuracy: 0.9744 - val_loss: 0.7629 - val_accuracy: 0.8280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLebDbGkCr3m",
        "outputId": "ad526bed-c370-429d-cf81-d819dfb6ee8f"
      },
      "source": [
        "optim = tf.keras.optimizers.Adam(lr=0.0002)\n",
        "gmodel32fixlastn = Generic32Batch_hel_Nor()\n",
        "gmodel32fixlastn.compile(optimizer= optim, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "hist_lastn1 = gmodel32fixlastn.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "   1/1563 [..............................] - ETA: 9s - loss: 6.1474 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.0114s). Check your callbacks.\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.7573 - accuracy: 0.3811 - val_loss: 1.2757 - val_accuracy: 0.5401\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.2418 - accuracy: 0.5602 - val_loss: 0.9663 - val_accuracy: 0.6662\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0048 - accuracy: 0.6522 - val_loss: 0.8093 - val_accuracy: 0.7215\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8535 - accuracy: 0.7061 - val_loss: 0.7667 - val_accuracy: 0.7356\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.7318 - accuracy: 0.7502 - val_loss: 0.6773 - val_accuracy: 0.7675\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.6447 - accuracy: 0.7809 - val_loss: 0.6091 - val_accuracy: 0.7940\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.5625 - accuracy: 0.8093 - val_loss: 0.5789 - val_accuracy: 0.8022\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4950 - accuracy: 0.8321 - val_loss: 0.5865 - val_accuracy: 0.8007\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.4321 - accuracy: 0.8550 - val_loss: 0.5739 - val_accuracy: 0.8097\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3743 - accuracy: 0.8738 - val_loss: 0.5850 - val_accuracy: 0.8090\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.3287 - accuracy: 0.8879 - val_loss: 0.6079 - val_accuracy: 0.8020\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2802 - accuracy: 0.9040 - val_loss: 0.6410 - val_accuracy: 0.8119\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2469 - accuracy: 0.9161 - val_loss: 0.5730 - val_accuracy: 0.8225\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.2135 - accuracy: 0.9269 - val_loss: 0.5601 - val_accuracy: 0.8342\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1862 - accuracy: 0.9362 - val_loss: 0.6506 - val_accuracy: 0.8257\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1625 - accuracy: 0.9447 - val_loss: 0.6797 - val_accuracy: 0.8218\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1422 - accuracy: 0.9523 - val_loss: 0.6483 - val_accuracy: 0.8236\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1313 - accuracy: 0.9565 - val_loss: 0.6526 - val_accuracy: 0.8356\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1115 - accuracy: 0.9622 - val_loss: 0.6864 - val_accuracy: 0.8272\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.1058 - accuracy: 0.9651 - val_loss: 0.7144 - val_accuracy: 0.8269\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0984 - accuracy: 0.9670 - val_loss: 0.6878 - val_accuracy: 0.8350\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0907 - accuracy: 0.9686 - val_loss: 0.7481 - val_accuracy: 0.8258\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0826 - accuracy: 0.9728 - val_loss: 0.6897 - val_accuracy: 0.8397\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0810 - accuracy: 0.9731 - val_loss: 0.8081 - val_accuracy: 0.8252\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0753 - accuracy: 0.9744 - val_loss: 0.8161 - val_accuracy: 0.8302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhjQQL91EwGv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VcJKD1ZK-3h"
      },
      "source": [
        "# plotting the result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "dSeIRBFwLCNr",
        "outputId": "4ee29faf-129d-42db-f945-dd6888c3671a"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "\n",
        "# plt.plot(hist[\"accuracy\"], 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "plt.plot(hist.history[\"accuracy\"], 'r--', label='kaiming_init')\n",
        "plt.plot(hist_norm.history[\"accuracy\"], 'bs', label='normal_ini')\n",
        "plt.plot(hist_genbefor.history[\"accuracy\"], '', label='Fix_before_last')\n",
        "plt.plot(hist_lastn.history[\"accuracy\"], '', label='Fix_head_normal')\n",
        "plt.legend('lower right',)\n",
        "plt.show()\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAI/CAYAAADgJsn+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hc5Z328e+ZKk1RH1Vbttx7B1NCNx1CICShpfMmeXez2SybxrvZQCCE3eym192w2SQQSCc2hO5QQgvY2AbbGGNJrrJkq0szmn7eP56RRrIk24BsSdb9ua5zzZnnnDnnzECIfV+/5/dYtm0jIiIiIiIiIiInNsdoP4CIiIiIiIiIiBx7CoFERERERERERCYAhUAiIiIiIiIiIhOAQiARERERERERkQlAIZCIiIiIiIiIyASgEEhEREREREREZAJwjdaNS0pK7KlTp47W7UVERERERERETjjr169vtm07NNSxI4ZAlmX9DLgMOGDb9oIhjlvAd4FLgAjwEdu2XznSdadOncq6deuOdJqIiIiIiIiIiBwly7J2DXfsaKaD/Ry46DDHLwZmZrZPAD9+Kw8nIiIiIiIiIiLH3hFDINu2nwFaD3PKFcAvbeNFoMCyrIqRekAREREREREREXnnRqIxdBWwp9/7vZkxEREREREREREZI47r6mCWZX3Csqx1lmWtO3jw4PG8tYiIiIiIiIjIhDYSIdA+YHK/95MyY4PYtv3ftm2vsG17RSg0ZKNqERERERERERE5BkYiBFoDfMgyTgE6bNvePwLXFRERERERERGREXI0S8TfB5wNlFiWtRe4BXAD2Lb9E+AhzPLwOzBLxH/0WD2siIiIiIiIiIi8PUcMgWzbvvYIx23g70fsiUREREREREREZMQd18bQIiIiIiIiIiIyOhQCiYiIiIiIiIhMAAqBREREREREREQmAIVAIiIiIiIiIiITgEIgEREREREREZEJQCGQiIiIiIiIiMgEoBBIRERERERERGQCUAgkIiIiIiIiIjIBKAQSEREREREREZkAFAKJiIiIiIiIiEwACoFERERERERERCYAhUAiIiIiIiIiIhOAQiARERERERERkQlAIZCIiIiIiIiIyATgGu0HEBEREREREREBwLYhFYdkFJJxSMUgmdlSseHHktHM52KHHI8fcm5siOtn3n/8cfAXj/YvcEwpBBIRERERERGRw7NtSPRAPAzx7swWhlj3Ie+7MudkzktE3lpIk4qP0ANb4PKC02teXV5wevrte8HlAW8wO2ZZI3TvsUshkIiIiIiIiMiJJpWEeFe/oObQ8Kar39gQ7+Pdgz9np4/u3g4XeAImYHHnZgMXVw54/OAqygQyOWZ8QFDjHXYsZbmJW15itouY7SJqu+ixXfSkXfSknfSkXURSTrpTLiJJi2gyTU88RU/CbNHMfjTzviea7hvrSaR49OIARcf2n8qoUwgkIiIiIiIiMlLSaVPNkopDKjFwP50YenzI/eThz0lEDglqwgNDn1Ts6J/Z7QdvwAQ0noDZ/CEonJp9f8hx2+Mn5faTcPiIO33EHbnEHD6izhxitptYMk08mSaWCWL6gpdM6BJLZMOXnkh60PHe99HMWDSRJp5KA/HMdnQcFuS6neR6nOS4nQP2C3Ld5OR5+8acqgQSEREREREReRvSqX7hR9K8t1OH7KcO2U+aapO+/eHOGWK879rpI9yndzw99DlvK6jpN2anjsnPaVtOUz3jdGM73NhuH2m3j5QrQMrlI5lbSCLoI+n0EXNkQhmnj6iVS9ThowcvPVYuEXIJk0O3nUPYzqE77SGWgngqTSyRIp4y4U28M50ZS/eNxZJpYsmUOZ5KY9tRIAq0vuXvY/WGM24TyOS4HeR6zPtgjovSoLfvfU4mpMntPe+QsVy3k5x++7keJzkuJzkeBx6nA6t/uGPbEItBNEr5rCBNB519h/4t81pWBo2N7+gf15ilEEhERERERGQ8SCVN9UciYqo9EpFMH5XeCpPEwP10cmAI0xtUpBPmWn1hR/LYfB57tH+xQWzLgW05zSsOUjhJ4SCFg6RttgQuEpaLJC6zP2BzkyQn+952Ee93PG47SVgu4nbvcSdx20mczHm2ORbHaV5tJ7Hec3H2u87A9wlcpEdwcW/LiuNxJvG6InhcTrwuBx6Xo+/V4zSvgRxXZszZN+Yd4jwz7uzbv+Z9DuyUA5LmNZ1wYiec2EknzfVxclIJvMk4ViaMwe+H6mrzcI8/DpGIGY9GoTMGs2bB2WebgO/LX+4LcfpeL74Yrr8eurrgkouyn+09/k//ZLY9e7L3AZqG+Xe0qWnEfuoxRyGQiIiIiIjISOhtnNs/pIlHIBHONtTtPxaP9At1hhvrd60Ra5ib4XCBw52pLundz2y9+w5XZizTUNcbGOK8/tc55DO9+w5XZnOC5ez36gKHo99+v+MDzhlq3EnSdtAeTdHak6KlJ01zOElLJMXBcJKD4STN4SRN3UkOdCdp7kli24On+/g9TkqCXkoCXor8HjxOB1hgAQ7LwrIyr4DV9x4szP6hY45+YxYWTgv8FgT6jZlzstfFsg75fP/r9t7b7JN5Do/TGhjADAhk+r06nQMDnlgPrp5INoCJRs0PsWiReX3uOWjYlz3WHYXCQrj2BnP829+G2trs8WjUhDRf/7o5/t730vPmH4b9165g4XTYv3/g4DXXwH339X2erq6Bx2+80YRAlgXf/Ca43ZCTYzavFxYsMOe5XOZYMDjw+LRpmZsXwC23ZMdvOsL/Rk5ACoFERERERGTiSKf6hTHhgfuDxo4mpDlk7K1y+8HjM81z+/Z9EKwwr73v3T7Tj6VvLHOu0zs4uBlu/9BwZoz2P0mlbVrDcQ52xWjuNlvvvnmN9O23RuLYQxRz5LqdhIJeSgIeqkoCLKkxIU8o2PvqIRTIoSTowecZ5b8Wp9Om8sXnM4HYvn2wcyeEw9DdnX391Kcor7CGrFIp87bRGC00b778ZfjDHwZWwgSDsGuXOX7Nh+BPfxp4gZoaqKsz+7feCk88MfD4woVwQyYEevBB2LRpYMgSDPZ7mLLDf9+vfMU8V+9nc3LM/XutXWvCnP7He69vWeazw8nNhb/8ZfjjwaD5fr0mYAhk2UP9L+Y4WLFihb1u3bpRubeIiIiIiIxh6fTgaU+HhjO9S0/3D2Li4YGhTLx7YDVNPPzWmuWCCUz6BzDu3KHDmL6g5pCxAef6Bl9rjAYxIy2dtmmNxAcEOs1dcQ52x2juinGwbzxOazhGeoi/pua4HQOCnN79UMAzaMzvfWvBTnn50FOAhuwN091tgppweGBQc955pmJm3TpYs8aM9w9xfvITqKiAu+6CO+7Ijvf0mOseOAChkAlx7rhj8MOEw1h+37Dfoe+v9j/4ATzzTDakycmB/Hy4/XZz/M9/NoFQ/5AlPx9WrTLHd+zIhjT9N7//qH/Pw/1rPUoRxJDGy3O+VZZlrbdte8VQx1QJJCIiIiIiIyeVgJ42CDdDpGXg1tPWb/npyPCBzVutqHG4+4Uv/uy+rxgKqgeHMn3BjP8wY5lrOI//X5nimdWUIokkkbhZGSkcSxLJrJJkxpKE46ZBbyptY9s2KdsmlYa0bZNKm613PzuWOZ5KkUqmSCf7vVoWKW8O6bRNqq2dVDJJOmWun06nSbs9pAJB8/kDB0ml06RtSNmYV4+XlM9nPt/RQVsygG0BDhvLmcYaoqWNJ5UgFOumJN7NpPIili6aQ4nLJvQ/P6Ek3k0oHqYkYbbAJ27E+uhHoKEBrr46+zd4KzN/6rOfNVOJ6urg4x8feMyy4AtfgAsugK1bTX+Y3nGgqenhIf9ZNDUBxcUmsHnoITj3XFMJc+21g09+8UVYudJUyXztayY08fshEDCvvWFPZSWccUb2WO/xnBxz/IMfhDPPHPjZQCB7/Eg+/WmzDefSSw//+Rkzju4+Mi4pBBIRERERkaHZNkQ7Boc5kZZMyNPabywT+kQ7hr+eJ2h6yvQPWXLyIK/iKIKawBDVNZnznO7j95tkJFPpAaFMOJakJ9EvoImlMsf7BTn99vuP9b6PxM01Eqm3V4LgtNM4XS4cDnDGYziSSTOWTuNIp3C6XDhLQ+b4nj04oj3Z43YaZyCAY85snBY4t23D6unBbafJsdM40mmcpSGcJ63AYVk4X3gNRzyG07JwOkwfG2fNVJxzV5rjq//GD/ZeCbaFnbYg5SAV9pAK55AKe9j6rs9TkowQTMWxbNv8u/auq+GqRdDZCT/dC04bcmzwOsEOgi/XfFGHI1uV0vtZ2zbjvWPp9MBjAKnMql3JJHR0ZMePVPJx7bXmfpMmmfenngq/+tXgkGb6dHP8Ix+Bj31s+DKTSy4x23BmzzbbOFZWNnxl1VgyXp5zJGk6mIiIiIjIRJHoOXKI0zsWboaeVrMq1FCcHvCVgL/YVNz4is17XzH4isyrvyR7LLcIXJ7j+32HEEum6OxJ0tGToKMnQWfmtf9+ZzRhQpxMtU1Pb0ATN8FOJGaW0X4rvKTw2Sl8hXn4PC58rQfJbW7CF+shNxbB39ONLxkn9+8/hc/tJPc39+J/6QV88Si5iSi+RBRf0I/v8Yc5510uDuzPrLaUckDaAqyBfWE+/3nYvj073cfrNRUeX/yiOf4//wOtrQOn+0yaBOecY46/9JIJUvr3fcnLM1UxYAIVp3PQ9+xvPE21GS/POl6eU0bX4aaDKQQSERERERlvknEzfSrWCbHuTLVO/+lXrYdMx2o1x4edZmVlg5uhNn//cCez7/GPSj8b27aJJtLZ4CaaoCOSDXL6j3f2DB6PJg4f3uS6HeTluvGnEvg7W8lNxvElYvjiPfhiEXwXnk9uYRDfSy/ie/jP+KJhfIkouYkYvkQU/+o/kltShO/738H3/e9kApwYTqfDNKVtaDChyre/bXqzBINmCwRMX5Y77zQP8uyz5tzeY8GgCWGmTRs3QcB4eU4YP886Xp5TRpdCIBERERGR0ZZOQazLbPHuzH4mxBkwPtxYV3b8SM2NPYFhgpyiIap2iiG3wCy7fZzYtk04njLBTOTIwU12LElnT+KIVTgBr4t8r5M8K0m+nSQ/FSM/Fiavp5P8k5eTX11B/pZN5H33W+S3NpEXDZMf7WZhdAtN6cpB1ytzNdN48hUmiLnrLlMx89RT8NhjA0OaQMD0W8nNhYMHTR+Z3mNe74j9fuMlCBgvzwnj51nfUgNrmbDUGFpERERE5O1Ip03z4t5AJpYJZOL9Qpr+21DjvWNH2+zYlQPeoNk8AfDmQV7V4DFvIDvmzcsGPblF4D7KBrJvQyKVJhxL0hU1W3csSXcsMfB9NElXNEFX374Z74r2hj1JUkMt/5RhWZCX4yYv10V+rpv8XDflATf5dpK8YA75RXnkxXvIf+5p8tubyW9pJP9AA/n79xC8/VZc77nULBN93sUDL5yfDxf+1vR08XXAOYvN354zW9O7BwdAAE3JEnjuuYGDZ59ttuGEQmaTcWG89IZR0CPvlEIgEREREZk4Ej2ZaVLNEM70wel739yvV07meKwTOIoyAId7YCDjDUCgFIqnZ0Kb4MBtqCCn97xj1OQ4mUoTjqXoygQ2vWFNZzTRt989INwZeF5XJsQ50nQqAIdlqnGCOW721LmIR1zYMQ/puI901E066sbncvPN2zAVOuEO8jtbyW9vJm/BbIJnnI6j+aBZ6ampyWydnebi3/oWXPVPpt/Nhf8ABQXZIGfuTCgsMOctWQIPPJA9Vlo6cHWlxYvhe987Br+09BovwQooXJGJQyGQiIiIiIxPtm2WGj+qQOegOScRHvpaDlemyXFmilTlUvM+t+AwIU6/wMc1clN9Dv+VbbpjSQ52xczWbV7bwnE6+wU2vVU3/StxehKpI17fyoQ3eTluAl4XgRwXRX4P1UU+gm4HAYdN0OskUBAkkOMi2LCbYLyHQDJGIBElGI8QnFxJ7vnnYlkW3HYb1k+/MuS92oFr/jLEHJybboKz3mWmUTmdsHSpmQPTG+acdpo5b8YMiEaHn2ZVVASXXXaUv6wcCwpWRMYehUAiIiIiMjb0X458QJgzVMiTeZ+MDn0tpzcb6PhLoHhGv5WsesOefqFPTv6oNDnuFU2kaM4EOs3d8X4hT3RQ4DNUJY4FBDwOgi6LgNtBIN9Pgc/DpFSEoKuboD9BIBUnkIwR9HkIXnQ+gRwXgXvvJrjjDYKRTgLhTnzdHVizZ5uVowDOOAM2bIBIJNsY5YIL4NFHzf6Ud8Hu3QMf5qqr4ILzzP4PfgAMHQIB8J//CYWFJtzpDXpKS80xn8/03RmOwzGifXbGk/FUYSMiY4tCIBERERE5dmzbrEzVtR+6GrOv4YNDVOy0QDox9HXc/myAEyiD0vnDBzr+ElOpM4qhDkAqbdMajg8IcAYGOtmApzM69DLsRTlOShwpQnaM5ckIoUgHoXg3oY9dTyiQQ+jrtxL6zd0U9HTh6J22VlkJ+/aZ/UsvhYceGnjRWbPg8x8y+5tfgE2bTOCSm2teA4HsuZdeCitXZo/l5sK0adnjd99t+ib1/3xBQfZ4UxM4DvMj/fM/H92PKQOowkZE3i6tDiYiIiIib51tm2bHfcHO/sFBT+9rKj748978Q0Kc4kPCnEOqdty5x/87DsG2bbr6T8c6pEqn//uW7hhD9T72uyxCebmE8rwm1GnYSaizhVBbE6GD+8z7556kuCiI+7P/mKmmycjLMyHPli2UVzqGrgYpjNHYmqmQ2bkT4vGBIU5OjqmiOU7Gy6pLoJWXROTEoNXBREREROToJXqGDnQ6Dxkbqr+ONw+C5WarPhWCFZmtvN9r+XHroXM0ookUreE4reE4bZHMazhOayTRN0Wrf7gTTw6ejuWyIOS2CVkJKpI9LJo7nVDpZEJvbiH0h/sINewi1LSHknA7/kQU6uqgpgb+/d/hp18fOB1qwWmQ4wCnAz7zGbjhhmw/nNxsGDZUWAHQ1Nbvt506dYR/rRObgh4ROdEpBBIRERGZKFIJ6G4yAU5nw9BVO137Ido++LOunEyAUwkVi2HWRQODnbxKM03LGxj82eMonkzTHonT2hfmJGiNZEKd/iFPJHMsHD9sw+RU2EMq4iUV9uKJevi/5S8Taj9I6OrLCS2ZT+jZJwn934+TH+3OTscCePZZOH02WLuhxIL5K6Hs3dmgp3fp8C98Ab74xeG/0MyZI/TLjB71rxERGTs0HUxERETkRBBphfbdhwQ6hwQ94WYGLXfucEGgPFuh079qJ69fFU9OwXHvsZNK27RHeoObxBCVOtmKnbbMWFds6N46AEGSFNlxChM9FFWUUDiliqJYF4W/voei9oMUtjZR1NXGadEXSUc8pKNusAdOm7JnzzHpxde/DqefbqZbPfFEtlKnvHzwUuTHyHiaZiUiIsePpoOJiIiIjHfpNHTug7Z6aK2Htp399uvNqloDWOAPZat3KpeZap1Dgx5fyXHrDxOOJWnuNqtfDQxx4rR1x2ntjNDWFTVjcZuOeGrYMMNnpSnM91OU46Rw/YvUdLVR2N1KUaSTwp5Oii6/iMIPX09RpJ3CU1dQ0NONJ52EYBDy8+HWW+H9l8D+/fBIF0wpg4LZkJ9P8kvB4b/Etm0D30+dCjfeOFI/kYiIyDGlEEhERERkrEj0QNuugeFO206z375rYINlhwvyJ0NRDVQth8KpUDgF8qpMwBMoA6f7mD6ubdt0RpO0ZIIdE/DEaO7o4WB7mOY45n1TG81xmx576LDJk0pQHG434U1PJ5WRToqmVFJ4xSUU+jwUfurjFHW2UJiKUuS2KPQ6yfnwDfClL0EqBZ+6B8ryze9RUGBCnqVLYXoxpApg8wYzlpcHTufAm1dUwK9+NXDsS8foBxMRERllCoFEREREjpfe5dIPreLprezpahh4vicAhTVQOgdmX2wCn8Ia85o3CZwj/0e5dNqmoyfTELk33OnsoflgB82tXTR39NAcjtOcsGi2XUM2SXakUxQlo5RUl1MS8DKldgMlDbso6emgxJmm2OugZOl8Cv/lCxT5PeR+7XasnLQJagrKIX82zJgBS2eZCz5xnwlwhppi5XTCT386/BdyOk/Y5sjqtSMiIm+VegKJiIiIjKR0Cjr2DjNtayfEOgeeHygfGO4UTs3u+4pHpA9PKm3TGu5XqdMVo7m5g+akwwQ9ew/Q3B6mJQEtaRdJa3DFjiuVpDjSQUm4jZJIOyWOFCUfvpZQwEvJr++mZE8dJUEvxUUBiorzcc6ZDddcYz68ezcEApTPK6SpafD3GWvLb6vXjoiIjGfqCSQiIiIykuKRwdU8fdO2dkM6kT3X4TbTtAqnwuSVAwOfging8b2zR0mm2dUSprahnZ07G2lu6TTVOt1xmvOKaQ7Hae2OkWZwsuFxOUyI03yA8p1vsiDSTkkqSokzRYnfQ8m/3WaOP/U4+U37cJSGoHShaXzcuwGcccvhH7K6GjjMkubDjI8WVdiIiMiJSiGQiIiIyKGSMdOEuWMvdOwzwU7/wKf7kITAmw9FU6F8Icx798BqnrwqcDiHusvRs206Gg6wY+tOanc2UdvURW1ngrriyezqiJFKZ8tTcuNRU6kTbmfyGaUsnVJO6PVNlKx/kZJcJyXBHEoKfJSECgh++lNYXq8pw0mfCyUl4PEMvv8Hrnpnzz/OjKWqJBERkZGkEEhEREQmllQSuhtNuNOZCXn6Ap+9Zj98cPDn8qpMuDPjfBP49E3fqoHcwnc2bSuZhL17Se/aRcOOvdTubaG2OcKOpadRm3BTu6eZ5mTvFC0nnqSfmvYG5lTD9qem0747QKIlQKLNhx03zaDLSlI0/pxMI+SFwA3D37+8/O0/u4iIiIwb6gkkIiIiJw7bhkgLdOwZOtzp2Add+8FODfycJwj5VZA/yYQ9fa9VZsWpvCpwD9GU+GjF4/DGG6Y3zq5dRHfvYef+DnZcdBW1oSnUbq6lduMb1BVNoseTvU++C2ZUFTLdm2LG/lqmlxcwfUYFk+bU4CoNgWWNq/414+lZRURExiv1BBIREZETQ7SzX6Czd3DI09kAyejAzzi9JszJq4KaMwaHO/lVkJP/zp4rkYCXX4Zdu/qCHnbvpvXqa6k9+2JqN9dR+60fs6N4MrVFk9hTcBp2hQM2xbCs7UzK8zJ92mROKc5l+uRips+cxIzJxRT5PVh9yckZ7+wZRUREZMJTJZCIiIiMDYnoIeHOPlPR07vfuW/wylqWA4KV2ZAnv8osnZ4/KbvvLxmRFbZ49NFBIQ8XXkjqSzezr7GNHWdcSG3xZGqLJ1FbOpUdxZNo8/j7Pu61bKbluZhelseMycVMLw0yPRSgpsRPruft9wwaT9U15eXDN1xWHx4REZGRcbhKIIVAIiIicvyk09BaBw0bYP9G02i5ty9PpHnw+f7QIdOz+oU7+VVmeXXnCBU2P/OMmbJVX5/dliyBn/wEgEhFFXXpHGpD1dROnUdt+TRqy2uoI5d4Mt13mRK/m2mZgGd6yM/00gAzQgGqCnJxOEYgjDrEeAqBRERE5NhTCCQiIiLHn21D+y4T+DRsgH2vwP5N2WoeVw4UTRs63Mmreud9eA71yiuwefPAkKe4GP74R3N86VLsjRvZX1hO3ewl1E2dR93MhdROmUvdwTD72nv6LuWwoLrIx/RQgBmlARP4lPqZVhKg0D/E6lrHkEIgERER6U89gUREROTYsm0zXas38OndetrMcacHyhbAwvdB5VKoWgYls0euigdg+3bYuBHq6rIhTzRqKnwAbr0VHngALItwdQ31c5ZSO3U5dY9vp/ZgN3XXfIP6SxP0JLPJSbrLSeKZBMnWQhItk0m0BsgjwM7NPryud7js+wgpKxt+ipWIiIhIfwqBRERE5K3rahoc+IQPmGMOF5TOhbnvNoFP5VIonQeud1ghs3s3bNgwsJJn927TkNnthu99D374Q3NuSQmpmmk0zF5I7bYD1DWHqbvy89Sd91nqwmkaO2N9l7X+8iaTCnOZXprPKfMCTAv5mRbyc+aSAKluLzCw1CYCeMfQn6DUS0dERESO1hj6I4yIiIiMSeEW2L8B9vULfLoazDHLAaE5MPP8bOBTNh/cuW/9PgcPwvr1JtzZuTMb9Nx/P0yaBPfeCzffbM4NBKCmBmpq6Gxppy7hpu6yj1G38gPUpTzUtceobw4TS6bh5y8DkJfjYloowGkz/EwPBZhW4mdaKMCUYh857sFVPanut/l7iYiIiIxRCoFEREQkq6fdNGzuX+HTvjt7vHgmTH1XNvApXwjewNFdOx6HHTtg27bsVlcH3/kOrFgBjz8O119vzvV4YOpUE/T09JBMpdlz6dXUzXsXde586iI2tc1h6g6Gaf7OS323cDosqot8TCvxc8bMEqZlwp7ppQGKByy3LiIiIjLxKAQSERGZqGJdsP/VgYFPa232eOFUqFoOJ90IlcugYhHk5B/5ui0tA4Oe97wHTj8dXnwRzjore15VFcycCYmEeb9qFW1rn6Yur5xaO4e65gh1B7upe6CBXS1vkkj19urpoNDnZloowLlzQn1Bz7RQgOoiHx6XY6R+IREREZETikIgERGRiSAegcbXBgY+zduBTLCSNwmqlsLS602FT8US8BUNf71k0kzZ2rYNKipg+XJoaIBFi0wI1MvrhenTTQi0eDHccw+xmbPZE5pMbcSm7mCYur3d1P34eeoOdtMWSQBdALidFlOK/Uwr8bNqbhnTQn6mh0ZnBS4RERGRE4FCIBERkRNNMgZNm/sFPhvhwOtgp8zxQJmp7Fnw3sy0riUQKB36Wp2dZps0CdJpuOYa2LoV3nzTTO8C+OQnTQhUVgZXXw2zZpGaPZuGSdOp9xZS39pD/Zot1DeHqW+uZO/mJtJ2djmrkoCXaSE/Fy0oZ1pJdqn1SYW5uJyjV9WjVbdERETkRGPZtn3ks46BFStW2OvWrRuVe4uIiJww4mFo3Az7N0HjJvN6YBukM1OscovMcuy9PXwql0KwAobrjfO//wvr1mWncjU0wKWXwoMPmuOrVpmmzLNnY8+eQ8u0WdQXT6Y+alHXHKa+uZv65jA7WyLEk+m+y/o9TmpCfmpKAtSUmOqeqSV+akr85Oe6j/GPJCIiIivf8xUAACAASURBVDJxWJa13rbtFUMeUwgkIiIyTkRaofFV08dn/yaz3/wmfVO6fMVQsRjKF5mwp2oZ5E8eGPhs3w4bNw7s2ZObC889Z46feSa8+irMmdO3dS9Zzs6FJ5uQ52A26KlrDtMVTfZd2u00TZlrSswy6zWZkGdaiZ9Q0NvXlLm8fPgKGy13LiIiIvLOKAQSEREZb7oaTdCz/1WzWlfjqwNX6cqbZBo194Y+FYshr9IEPgcOwOuvZ0OevXvhd78zn7vuOrjvPnPelCkm6FmyhPjtd7C7NUL93mbqu1LUt5iVt+qbwxzoivXd1rKgMj+3L+CpKfFTEzJBT1XB0U3fOtwCXaP0xxIRERGRE4ZCIBERkbHKtqF9Vybw2ZSt8gkfyJ5TND0b+FQshvLF4C82x9Jp06NnzhxwueCWW+C227Kf9flg9mz4619J5/rYv+416tuj1OcUUtcRz/TpCbOnNUK63x8Jiv2eQSFPTUmAKcU+ctzOd/SVFQKJiIiIHDuHC4HUGFpEROR4SafM9K3GV7OhT+OrEO0wxy0nhObAjFXZ0KdsAeTkZa8RjZqePc8+a7bnnoP2dnj5ZVixAi68kNZAIXVT5lCXX0592kt9c4T6/17PzpYwsb4+PQfxeZzUlPhZWJXPFYsrsz17iv3k+9SnR0REROREoxBIRETkWEjGzIpc/QOfpi2QiJjjTi+UzYf5V2UDn9J54M4deJ3mZnhsjanmmT3bhD6rVpljc+aQvvpq3lxxFus6fKz/zUbW7Yqxu3UmtKSAfbgcFtXFPqaV+DlzVkm2MXPIT2m/Pj0iIiIicuJTCCQiIvJOHWmFLk8QyhfCsg9npnQtgpJZ4Byi2iYWg1//Olvps22bGf/KV+CrXyWy7CQ2/XI16wunsO5gjFd2tdFZn4T6eor9HpZPKeT6ldXMKgtSU+If9WXWRURERGTsUE8gERGRt6KnbeDqXPs3Db9CV28Pn8IacAwRxCQSsGGDqe7Jy4OPfxySSSgsNP19Tj+dptPOZt2M5axzF7N+XydbGzpJZpr3zCwNsGJqIcunFLFiSiGnLPDR1DS4smesrbql1cFEREREjh01hhYREXk7Yl3QsBEaXoF966FhwyErdFUdEvgsMmNHmmL1ne/AmjXwt79BJDM97NJLSa15gO1NXax7ZQfruyzW7Wpjb1sPADluB4snFbB8SiErphayrLqQAp9nwGXVcFlERERE1BhaRETkSJJxOLDFhD37XjHbwW30VfgUTIHKZbDiY9nQx19y+Gvu2WOqfJ59Furq4KGHzPjLL0NnJ+EbP8nGRaezrmgq61qTbPzqY3TFkgCEgl5WTCnkI6dNZcXUIuZV5OFxaVqXiIiIiLx9CoFERGTiSaehtbZf4LMeGl+DVMwc95VA1TKY/x6oWg6VS48c+KTTphTHsuBnP4OvfhV2Z6qG/H449VQaGlpYfzDG+vffzLrdbby+v4vUmzaW1cjssiCXL6lkxZRCVkwpYnJRrpo2i4iIiMiIUggkIiInvs6GbNizb72Z4hXLLMvu9kPlElj5CVPpU7UcCqqPPKUrEjEVPb3LtD//PDz9NCxeDMXFJFeewrbP3Mz6SfNZl/Sxfnc7Dd97EYBct5Mlkwv4u7Ons3xKIUurC8nP1ZLsIiIiInJsKQQSEZETS0+76d3T28Nn33ro2m+OOVxmWfYFV5mwp2o5hGaDw3n4a9o27N0LOTkQCpnQ5+yzTRNngPnz6frA9WxsSbPu8e2sP1jOhjkfJXwwBQc7Kc+Ls3xqITdWm34+cyvycGvFLhERERE5zo4qBLIs6yLgu4ATuMu27X875PgU4GdACGgFbrBte+8IP6uIiMhAiaiZxtXQr8qnZUf2ePEMmHpGNvApXwDu3CNfNxaDX/4SXn01u7W3w+23w5e/jD1vHvv++V9YP/sk1vnKWNfYwxuNnaQfa8BhwezyPK5aNimzclchVQXHZ2pXWdnwq26JiIiIiBxxdTDLspzAduB8YC/wMnCtbdtb+53zO+BB27Z/YVnWucBHbdv+4OGuq9XBRETkLUmnoHl7NuzZ9wo0bYZ0phonUJ4Je5aZrXIp5BYe5nppqK/PhjyvvQaLFsFXvmIqfIJBs0z7okWEFy1l26ylvDp5HuviXtbvbKOxMwqA3+NkabUJe8zUrgKCOZraJSIiIiKj452uDnYysMO27brMxX4NXAFs7XfOPOCmzP6TwJ/e/uOKiMiEZ9vQsWfgSl37N0K82xz35pmQ57TPZEKf5ZBXOfz1OjpMyNPZCZdcYsYWLYItW8y+ZcGMGdhz5tLUEeX1/Z1sve85tnbbbG3sYmdLGLsJaGqlMj+Hk2qKWJEJfeaUB3FpapeIiIiIjANHEwJVAXv6vd8LrDzknE3AVZgpY1cCQcuyim3bbhmRpxQRkRNbpPWQxs2vQPigOeb0mCXZl1yXndZVNB0cQwQv6XR2/K67YM0aU+Wza5cZq6kxS7UDyX/4DHVJD1vLprHVXcDWgz1s3d9J651r+y5XXeRjXkUeVy6tYl5FHvOr8qjIP4rpZCIiIiIiY9BINYb+HPADy7I+AjwD7ANSh55kWdYngE8AVFdXj9CtRURkXDr4Bmz5E2z9ExzoLS61TKPmmRdkpnQtg7IF4PIM/nxrK2zcOLBvT309HDgATids2mQCn9NOo+uTf8e2qfPZml/F1t+/ytb9nbzRNJl4Mg17uvC4wswuC3L+3DLmVeYxrzKPOeVBgjluysvhv4bps9PYeEx/IRERERGREXU0PYFOBW61bfvCzPubAWzbvnOY8wPANtu2Jx3uuuoJJCIyAR3cbkKfLfdngh8LppyWCX2Wm6XavcGBn4nH4Y03skHPF78IRUVw221wyy3mnLIyWLQIe9Ei9v/Tl9jakWJrQwdb93fxemMnu1oifZcr9LmZX5nPvMo85lYEmVeRz7SQf9jVug7Xz/kI/xcqIiIiInLcvdOeQC8DMy3LqsFU+FwDXHfIDUqAVtu208DNmJXCREREoPlNU/Gz5X44sAWwoPpUuPg/YN67IVhuzrNtU1rjtyEvD556Cj7zGdi2DRIJc47HA+99L5x8MokPXMOOBSvZWjiJ18MWW/d3snV/J+3f/1vfrWtK/CyozOd9yyeZCp+KfMryvMdlpS4RERERkbHmiCGQbdtJy7I+DTyKWSL+Z7Ztb7Es6zZgnW3ba4CzgTsty7Ix08H+/hg+s4iIjHXNO0zos/VPZgWvvuDnGzD33ZBXYfr3rFsHa34AL7xgqnyam+Gee+D666GwECZPhksvpWP+YrZVzWSrM5+tu8JsffGvvNnUTTyVBnbidTmYUx7k4gXlzKsw07lml+cR8I7UrGcRERERkfHviNPBjhVNBxMROcE074Ct98OW1dD0mhmrPhXmvcdU/ORVQixmevlUVJhmzVOnmv49y5bB4sXYCxex74xVbPUUmsqeBlPds7etp+82xX5PX9+eeRV5zK/MY2qx/5it0KXpYCIiIiIynrzT6WAiIiJDa6k1FT9b/pQNfiafAhf9m6n4ya+CtjZY82dYvRoeeQQuuAD+8AeYMoXO3/+JZyrmsr4lwdaGTl7f30nnb8zqXZZlpnMtmVzAdSurTYVPRR6hoKZziYiIiIi8HQqBRETkrWmpzTZ3buwNflYODH56fepTZqn2VArKy+G669h5yXt54q91rH39AC/vdJNMv0mu28mciiCXLa7sm841pzyIzzP6/zdVVgZNw6wOJiIiIiIynoz+n65FRGTsa63LNndufNWMTToZLrwzM9WryizX/u2fwuOPw1/+Al4vLFpE8vNf4JV3XcJainhi2wFqXwgDrzOrLMCNZ0xj1dxSllYX4nSMzeoeLQMvIiIiIicKhUAiIjK01vpsxc/+TWZs0klw4ddh3hWQPwm2b4d//YaZ6rV7t5nDdfrpdO7ax9M9OawtOpWnmg/S/tcO3M5OVtYUc8MpUzhvThnVxb7R/X4iIiIiIhOMQiAREcnqC37+BPs3mrGqFXDBHSb4ceSbvj6+Flg8yTR5vusuuOACdv2/23liylLW7gnz0s+3kUzbFPrcnDu7lPPmlnHmrBKCOe6+W5WXDz/NStU3IiIiIiIjT6uDiYhMdG07Teiz9U/QsMGMVS2H+Vea4CfshDVrTLXPX/4CiQTcdBPJb/wHr+xsZe2W/azd0cqOA90AzCwNcN7csiNO89KqWyIiIiIiI0+rg4mIyEBtu7JTvfoHP+ffbnr8JHxQWgrpNMypNCU7M2bQ+Zl/4pmTLmStXciTdzxBeySBy2GxcloR151czaq5muYlIiIiIjJWKQQSEZko2nbB1tWZ4OcVM1a5DM6/DWZdBlv3wh9Ww4fOg2QSdu4Eh4NdP/gf1lpFrG21+Ft9K8kNPRT6kn3TvM6YVUJev2leIiIiIiIyNikEEhE5kbXvzgY/+9abscqlsOqrMP89UDjV9PS5YiW0tIDHQ+q8Vbxy4dU88eetrN3ezI4DAK3MKA3w8TNqWDW3jGVjeDUvEREREREZmkIgEZETTcc+M9Vr8x9hX6b3WsUSE/yUnQ5Pb4J/Xw13roJCYNIkui65nGfedTlr/ZN5sraNtv0JXE27WDmtiGtPrmbV3FKmFPtH9WuJiIiIiMg7oxBIRORE0NWUDX72vGjGyhfBqlth8rnw60fgC7+FF28yXZenTmV37T6eaPezdm8BL01+H4k6mwJfG+fMLuW8uaWcOSt0TKd5lZUNvzqYiIiIiIiMPK0OJiIyXoWbs1O9dj4L2FA6z6zqZc8CqxDOOgu6uiAUIjV/ARsuu5Ynpp3E2habNzOrec0oDXDeHNPfZ1l1AS6nY3S/l4iIiIiIvG1aHUxE5EQRaYVtD5qKn/pnwE5B8Uw48/PAbHjkJfjPn0B9PZxyCl1PPsMz9d2s/enTPLWzk9ZwHNcb3ZxcU8Q1J1dz3pxSppZompeIiIiIyESgEEhEZKyLdsC2h2DLH6H2L5BOQmENnP5ZWHAVlM2HD34QfvVlcLlovujdPPoPd/BI7mRevP1xEimb/Fw358wOcd7cMs6cFSI/V6t5iYiIiIhMNAqBRETGolg3bH/EVPzseBxSccifDKf8HTALHtsI3/sFvPApsCya3n8Djy6/jIe8Vby0p5N0E0wtTvDR02s4b04py6cUapqXiIiIiMgEpxBIRGSsiEfgzcdMxc/2xyDZA8EKOOlGKD0DfvUEfOeXsHs3uN00XHIljzxbz8MHdrBuVwrbDjI9lOLT58zg4oUVzCkPYllaxl1ERERERAyFQCIioykRhR1PmODnjUcgEQZ/KSy9Hqw5EFoMJ50Me/bAD69iz8VX8vA//gcPucrY2NANLzUzpzzIZ8+bxSULy5lZFhztbyQiIiIiImOUQiARkeMtGYe6J81Urzceglgn5BbBwqtN8POXN+B7v4W9e+GKK9h51694qDbGw//+KK/t74YDsKDKwecvnM3FC8qZFgqM9jcSEREREZFxQCGQiMjxkEpC/dOm4uf1ByHaDjn5MPfdsOBKqDkLLrkMHvs+eDzsuPwDPPxPl/MQIV7/z6cAWDy5gJsvnsPFCyqoLvYNe6vycmhqGjxeVgaNjcfo+4mIiIiIyJhn2bY9KjdesWKFvW7dulG5t4jIcZFOwa7nTMXP62sg0gKeIMy+GJxz4al6eGwtrF+PnZPD9l/+noeaLR62i9neHAFg+ZRCLl5QzkULyplUOHzw09/h2gCN0n/yRURERETkOLEsa71t2yuGOqZKIBGRkZROw56/mYqfrauhuwncPhP8FJ8Gf9oAP1oN++7C9nrZcsX1PPzgZh7eFaau2YdlwclTc/jqaTVcOL+c8vyc0f5GIiIiIiJyglAIJCLyTtk27FtvKn623A9dDeDKgZkXgGsezLwQFi2Ddeuw/+vTvHrlB3loxcU8nMhnd3sUxysHOHV6MR97Vw0XzC+jNKjgR0RERERERp5CIBGRt8O2Yf9GE/psuR/ad4PTA9PPg8nXwl8b4EdrYP89pD+5iw0338nDjT4e/sqD7OuM4WqxOG1GkL87dybnzyujOOAd7W8kIiIiIiInOIVAIiJvRfMO2HSfme7VWgcOF0w7B86+GWZfAqefCxt+QyrXx7orP8zDS1fxSCxI44+fx+N0cMbMEj57QTnnzyujwOcZ7W8jIiIiIiITiEIgEZEjiUdMf59Xfgm7nwfLAVPPgNAl8PwBeHATvHgNSRte+shneejaII9E/TSHE3jaHZw9q5AvLazg3Lml5OW4j/njlpUNvzqYiIiIiIhMXAqBRESGYtvQsMEEP5v/ALFOKJoOi/4OnmyCnzwMjQ+Q8Ad44cqP8vBvXuHRHW20hovJdTs5Z04xFy+o4Jw5pQS8x/c/tVoGXkREREREhqIQSESkv0grvPpb2HA3NG0GVy7MuBjmvg8WXQx//jOxn1/H81d9jIcWnM1j4Vw6okn8rzdz3twyLllYzlmzSsn1OEf7m4iIiIiIiAygEEhEJJ2G+qdN8PP6A5CKQ+VSmPZ/4OE6uP1e+FwlW0Knc1+8mtU3/ZauWIpgt4vz55Vx8cIKzphZQo5bwY+IiIiIiIxdCoFEZOLq2Asb7zXhT/tuyCmA5R+FDXG4437Y/TTh0goe/PTt3FuwlE3fexavy8ElCyt49+JKTptRjNel4EdERERERMYHhUAiMrEk47D9YdPrZ8dawDZNnkuvgvfdDO4cuPtatiw5nXv/4bus7sqhO5ZiptPDLZfP4MqlVVrVS0RERERExiWFQCIyMRzYZip+Nv0aIs0QrIQ5H4KXwvC51dDyZ8ILP8ADPQHuO+3v2bSvA2+7g0sXlXPdydUsn1KIZVmj/S1ERERERETeNoVAInLiinXDlj/CK3fD3pfA4YLZF0PJ2fD//hte+j643Wx+/8e4b+UVrP79brpjKWaVBbj18nlcuXQS+b5jv6S7iIiIiIjI8aAQSEROLLYNe1/OLO3+R0iEoWQWzPw4lJwBF14J7e2Erf/lgTv+h3tzp/FqUxhvE1y2qILrVk5mWfXgqp/ycmhqGny7sjItyS4iIiIiIuODQiAROTGEm81Urw13w8Ft4PbD9IugPgA/eBTe+Dac9QqbF5zLvS/tYfUF/0q4M8XsXMdRVf0MFQAdblxERERERGSsUQgkIuNXOgW1T8Irv4A3HoZ0AiadBJd/D+7fAjd8C1Ipus88hwc+9VXutSp47ftmha/LFlVy3cpqllUXqNePiIiIiIhMCAqBRGT8adsFG+4xy7t37oXcIpj1fngtDVfeAcXFUPtbNt/0FX415xzW7AwTbkwxu8ziq++ez3uWVKnXj4iIiIiITDgKgURkfEhEYduDZrpX3dNmrOZs8F0Iv10PT/4YHA66l13EmknLuK+pktccfnLqu7lsUSXXnqyqHxERERERmdgUAonI2Na42QQ/r/4GetogvxrO/hLUXApLzoT21TB9Oq/d9i3unXoqaza1E375NWaXBU3Vz9Iq8nNV9SMiIiIiIqIQSETGnmgHbP6DWeGrYQM4PTDtfGgqhQM5JgQCum/6PGumnMS97TlsbugkZ3tbX6+fpZNHtuqnrGz41cFERERERETGA4VAIjI22DbsfgFeuRu23A/JHiidC9M+Co/thTv+CLEYrFzJq7tauG/9PlYnlhHZGmdOuZfbrpjPFUuOXdWPloEXEREREZHxTiGQiIy+nc/CY/8KDa+AJwiLPwBLPwT3roW//yIUFNB94ydZfdb7uK/RZvOPXyTH7eDyRZVcewyqfkRERERERE5ECoFEZPQc2AZP3ArbH4ZgBVR9GNZsh/nnwqTl2NeEeK1oCvflzWL15iYi6zuYUx7ktitMr5+8HPX6EREREREROVoKgUTk+OtqhKfuND1/PH7IuwR+9ALUfh8qK+nq6Gb1i7u476XdbGkIkOtu4vLFFVx7cjVLVPUjIiIiIiLytigEEpHjJ9YNz3/fbKkYnPwJ+OazsPbX2CtW8Oo93+U+zxTWvNZIZOtm5pQHuf2K+Vyhqh8REREREZF3TCGQiBx7qSRs+CU8eSeED0BgGVz/I6iYS0/4fh641sMvw3lsfq2TXHcjly+u4LqVU1g8KV9VPyIiIiIiIiNEIZCIHDu2DW88DE/cAs3bIWcaPJqAF59iZ/Em7gnZ/G6Ln46eBLPK0qr6EREREREROYYUAonIsbF3PTz+r7DrOXCXwjMFpJ56lSdXvY+77/w+T7/hwPXmTi6cX84HT53CypoiVf2IiIiIiIgcQwqBRGRktdbB2tthyx/BH4JLvknLZ3/Bb8uW8Kt/vZC9MYvSlJfPrqrm2pOrKcvLGe0nFhERERERmRAUAonIyIi0wtPfgJfvAtvC3lHMqzet5hdvJnjwnEriKZtTqoq4+ZSpXDC/DLfTMdpPLCIiIiIiMqEoBBKRdybRA3/7L/jrNyHWRbLez0Ovz+Sny67ltd/uxO9x8oGTqvngqVOYVRYc9PHycmhqGnzZsjJobDwOzy8iIiIiIjJBKAQSkbcnnYbXfmumfnXuJXIgj7t3r+JHsz5Ax7k+ZpYGuP3UKVy5bBIB7/D/qRkqADrcuIiIiIiIiLw9CoFE5K2rfdI0fW58jc7CBfyw9B/5r2gVzgU2Fy2o4IZTp3LKNDV6FhERERERGUsUAonI0WvcDI9/BWrX0hnz862eD/KL6IWEgrn843nVXLdSjZ5FRERERETGKoVAInJkHfvgya9hb7yPnpSX7yav4X/ti1labPGDy5dxwcIKNXoWEREREREZ4xQCicjwoh0kn/k21os/Ip1O8bPkJfy85yJWlfp54IazmF1VONpPKCIiIiIiIkdJIZCIDJaM0/rMT8j56zfw2V3cnzqdP+R/hAuqynjs8pUEAyM35ausbPjVwURERERERGTkKAQSkT6pVJqta++m9IU7KLObeCE1l6f2reDsT9/E3UtrjkmjZy0DLyIiIiIicnwoBBIR2sJxnn7iAWZsvJOF9pvUJit4vv4cTj3jCk79yg3gdo/2I4qIiIiIiMg7pBBIZALbtKedh596hmVvfo/3OF6m1VHEpvqlzDvtWqbf8jGFPyIiIiIiIicQhUAiE0w0keKBTQ088PxGVh34BZ9zriVlOzm44p8JXfDPFHn8o/2IIiIiIiIicgwoBBKZIPa0RrjnxV2sfulNrk6s4SeuNeQ4YiTfyMV7xucIXfiPqvwRERERERE5gSkEEjnBPV/bzE+fqeOZ7U28z/kMj7h/R4G7DXunC2vZTXj+5Uvg9Y72Y4qIiIiIiMgxphBI5ATVFU3wtQdf5zfrdnOFdxN/y/0VJel9UHUSJE7F+tK/QM7ILfUuIiIiIiIiY5tCIJET0LNvNvOF32/C7mzgL/Z/MM3aBZ0u+OgvYN4VcAyWehcREREREZGxTSGQyAkkHEty58Ovc8+Lu/mg6wVu4Ye4XGnIuQg+998QyB/tRxQREREREZFRohBI5ATxYl0Ln//9Jlra2vhj0X0sizwO8QDc8FuYf/poP56IiIiIiIiMMoVAIuNcTzzFvz+yjZ8/v5NVOTv5fugucjvrYdr74bofgEtNn0VEREREREQhkMi4tm5nK5/73SZ2tXTz3x0/5Hz381jJMvjQaph21mg/noiIiIiIiIwhCoFExqFoIsU3H3uDu/5azyK7gQ2tX6agKgpFK+HG34CviPJyaGoa/NmyMmhsPP7PLCIiIiIiIqNLIZDIOLNhdxuf+90mag+G+dqBu7m+5CGsSjdc8A049RN9K38NFQAdblxERERERERObI6jOcmyrIssy3rDsqwdlmV9aYjj1ZZlPWlZ1gbLsl61LOuSkX9UkYktljS9f/4/e3caXldZ9m38XBnapvNIOpG2QoHS0plCaVFRBERkkEFAUGRwQEQRUHxE5AFFHgVkqoqIIgIyKb6oIIg4kJShTdJAB1o6sNNmd+5O0zFpstf7YaUkadI2QLJ3hvN3HDnWXve+2l7YfvF/3Pd1n/mL2SQrt/PKuL9wQd5zBINHw9dfhWO+7NXvkiRJkqS92u9OoCAIMoFZwCeAVcCcIAieCcNwYb2y64EnwjD8RRAEhwPPAiNboV+pU3pz1WaufnIeS9Zu5arSP3HFEbPJXBqHY66Ej30fsrqku0VJkiRJUhvXnONg04ClYRguBwiC4DHgNKB+CBQCvWs/9wHiLdmk1FlVVSe5919LmfXS2wyqquDFxddz8OQNEAysHf780XS3KEmSJElqJ5oTAg0DVtZ7XwUctUfNjcALQRB8HegBHN8i3Umd2MJ4Bdc8WcLC1RV8YelzXN/7YbKnhnDop+C0e6F7/3S3KEmSJElqR1pqMPR5wINhGN4eBMF04PdBEIwLwzBZvygIgi8BXwLIy8troT9a6lh21ST55b+XcfdLb9MnpwtPD/wvkzL/AN26widvhSkXNWv2T27u3m8HkyRJkiR1Ps0JgcqAA+u9D69dq+8S4CSAMAxfCYKgGzAQWFe/KAzDXwG/Apg6dWr4PnuWOqwla7dw9ePzeDNewZndE9wydg5dSx6EA8fDmQ/AoEOa/Xt5DbwkSZIkqb7mhEBzgNFBEIwiCn/OBc7fo6YU+DjwYBAEY4BuwPqWbFTqyKprktz/8gp+9sJb9Ny5jYcLbmXm8eugZAsc8/Xa4c9d092mJEmSJKkd228IFIZhdRAEVwDPA5nAb8IwXBAEwU3A3DAMnwGuBu4PguAqoiHRF4Vh6E4fqRmWrd/K1Y8VM6+sgpMX5/OTqt/T8+Qd0GMgnPEQHPSxdLcoSZIkSeoAmjUTKAzDZ4mufa+/dkO9zwuBGS3bmtSx1SRDfluwgp8+v5icIMl9L/yIE07YSNBtCxxyMpx6L/QYkO42JUmSJEkdREsNhpb0HryzYRvXPlbEnFUVHD8ml9smlNG3exnUVMKJd8DUi5s1/FmSJEmSpOYyBJJSKJkMeeiVd7j1bwvosmM7d/335ouZhAAAIABJREFUPk6dfDDBnx+D3CPgrAdg0KHpblOSJEmS1AEZAkkpsnLTdq59dA6vrtrKR5cVctumvzDwvEpYOBemXwEfv8Hhz5IkSZKkVmMIJLWyMAx55LVSbvnbIjK2beUn/36As0/vRhDEgH5wwZ/g4I+nu01JkiRJUgdnCCS1orLyHXzn0bnkl1Yw8+CB/DS5nCGHJGHNS3Cow58lSZIkSaljCCS1gjAMefL1Um7+cwk1lbv40ZgunH/sJoL/dztUbYNPOfxZkiRJkpRahkBSC1uzeSff/d1s/hXfwdGxBdxWWczwo/rDH/7g8GdJkiRJUtoYAkktJAxDni4u48Yni9hVtYv/nfMkF35xChnbS2HR83D01+D4Hzj8WZIkSZKUFoZAUgtYt2Un33t6Pv9YuJapOUlui7/AyB9Mgdk/hZy+Dn+WJEmSJKWdIZD0AYRhyF9eX8ENfyphR0YW139qLF+c0J3MZ34PLz8Gh5wEp82CHgPT3aokSZIkqZMzBJLep41bK7n+vn/y3PqQiauXc9vQrRyc2wV+eTlUbYWTb4MjL3X4syRJkiSpTTAEkt6HuUvW8JX7C6gIM7lu8fNcevVZZG17Dh49B3LHwZm/hgPGpLtNSZIkSZLeZQgkvUfPlMS55vFihm3eyKOD1nPIrMvgL5fD+kVw9OXw8R9Adrd0tylJkiRJUgOGQFIzhWHIz/+9jJ8+v5hpo/pz3wWj6VfxEvzuJOjWBz73Rxh9fLrblCRJkiSpSYZAUjPsqknyvT+W8ERRnDMGhdx6wRi6/vkyePsFGH1iNPy556B0tylJkiRJ0l4ZAkn7sXnHLi7//VwKlm/iG/mP8s3zpxE8dg+smgOf/ClMu8zhz5IkSZKkNs8QSNqHlZu288Xfvk5sXQV3/O1OPvPlU6Dbc7ByDpz1Gxh7erpblCRJkiSpWQyBpL2Yt7KcSx+cQ1XFFh56/EamX/N56PECrHzdAEiSJEmS1O5kpLsBqS36+/zVfPa+V+jeNYs/7XyV6Vd9Dnq+WBsAPWAAJEmSJElqd9wJJNUThiH3v7ycHz/3FpNye3D/pdMZkD0VHjknCoDO/DWMPSPdbUqSJEmS9J65E0iqVV2T5Ht/ns8tz77FydtiPHrnxQzYvgEe/SysfA3OvB/GfYbBg6M50Hv+DB6c7v8CSZIkSZL2zhBIArbs3MUlv5vLo6+VcvmWhdxzzxV0+9IX4G+XQekrtQHQmQCsXdv077G3dUmSJEmS2gKPg6nTi5fv4OIH5/D2uq3cWjGXc39xI9z0fRjyWm0A9Ot3AyBJkiRJktorQyB1avPLNnPxg3PYUVXDg92Xc+yPb4QffA+GF0YB0GfuNwCSJEmSJHUIhkDqtF5cuJYrHyumX/cuPHz5URySMx0OCKDLcxCbHQVAR5yV7jYlSZIkSWoRzgRSp/RgwQq+9Pu5HDyoJ093e4tDemZAt8zaAKgAzviVAZAkSZIkqUNxJ5A6lZpkyM1/XciDs9/hhMNzuev1h8iZdQ/khNC1XgA0/uy9/h65uU0Pgc7NbcXGJUmSJEn6gAyB1Glsq6zmG48V8+KidVx27Ciue+E+MmfdA9d8s14AdN8+AyCANWtS1LAkSZIkSS3IEEidwtqKnVzyuzksjFdw82ljufDJu+Huu+Gqr8O4pfBOAZz+Sxh/TrpblSRJkiSpVRgCqcNbtLqCix+cQ8WOXTzwhSM5rk8NPPIIXHk5TFgBK16OdgBN+Gy6W5UkSZIkqdU4GFod2r8Xr+PsX75CGMITX57OcYcOgiFDYM5smLzSAEiSJEmS1GkYAqnDeuS1GJf8bi55/bvz58uPYezPfwLXXQdV2+Hf34IV/4UzfmkAJEmSJEnqFAyB1OEkkyG3PLuI7z09n48cMognvjKdwXf8GG65Bco3wB/Og+X/gdN/ARPOTXe7kiRJkiSlhDOB1KHsqKrhqsfn8fcFa/j89BHccMrhZP3wZrj5Zrj0IjhmA6z4D5z+c5h4XrrblSRJkiQpZQyB1GGs31LJpQ/N5Y1V5Xz/lMO5eMZIgh//GG68Eb54IRybqN0B9HOYeH6625UkSZIkKaUMgdQhvL12C198cA4bt1Zx3wVTOGHs4OiL0aPhki/Ah8th+b/htFkGQJIkSZKkTsmZQGr3CpZu4DO/mE1ldZLHv3x0FAAtXRp9efqn4SMVtQHQvTDpc2ntVZIkSZKkdDEEUrv2xJyVfOE3rzO0Tw5PX34M44f3hdtvhzFjoOC/8Nj5sOwlOPUemHRButuVJEmSJCltPA6mdimZDLn9H4uZ9a9lHDt6ILM+N5ne3bLhZz+Da66Bz54Jy34Gy/8VBUCTL0x3y5IkSZIkpZUhkNqdnbtquPapN/hLSZzzph3ITaeNIzszA+6+G771LTjrdDg5CctfglPvNQCSJEmSJAlDILUzm7ZVcdlDcymMJbjuk4fx5Q9/iCAIID8fvvEN+MypcGoGLHvRHUCSJEmSJNVjCKR2Y/n6rXzxwTms3ryTWedP5lPjh9R9OWMGPPAryH4uCoA+fTdM/nz6mpUkSZIkqY1xMLTahdeWb+SMn89m685q/nDZ0XUB0EMPwVtvQU0VdH0Blv0zCoCmfCG9DUuSJEmS1Ma4E0ht3p+Ly/j2U29wYP8cfnvRNPIGdI++eOABuPRSuOhCOH4HvP0CfPouAyBJkiRJkppgCKQ2KwxD7v7nUn724hKmf2gAv7xgCn26Z0dfPvggXHYZfPIE+Ng2ePtFOOVOmHJROluWJEmSJKnNMgRSm1RVneS6P73Bn4rKOHPycH78mSPoklV7evGhh+Dii+ETH4PzekYzgE75GUz9YnqbliRJkiSpDTMEUptTvr2KrzxcyKvLN/GtTxzC1z92cHQDGEAyCb/9LXz8o3BBX1j2j9oA6OK09ixJkiRJUltnCKQ2JV6+gwseeI1Vm3Zw52cncvqkYXVfhiFkZMDTT8FfvgLLXoBP3WEAJEmSJElSM3g7mNqMZDLkqsfnsa6ikocvPaphAPTEE/Cxj0H5Rnj2a7UB0O1w5CXpa1iSJEmSpHbEEEhtxu9fjfHaik18/5QxTBvVv+6Lp56C88+H5C74y5dhyXNw8m1w5KXpa1aSJEmSpHbGEEhtQmzjNm597i0+csggzpl6YN0XTz8N550H06fBpUOiGUAn3wbTLktfs5IkSZIktUOGQEq7ZDLk2iffICsz4NYzj6gbAv23v8E558C0KfClYdERMAMgSZIkSZLeFwdDK+0enP0Or7+ziZ+cNZ4hfXLqvhg9Gk79FJzeBZY9bwAkSZIkSdIH4E4gpdWKDdv4yfNvcdyhgzh7yvBoccGC6CawD42Es3tEAdAnf2oAJEmSJEnSB+BOIKVNTTLk2idLyM7M4MefGR8dA5s9G447Dn50Mwwvgbf+GgVAR30p3e1KkiRJktSuGQIpbX5bsIK5sQS3nT2BwX26QTIJV14Jgw+A3Lnw1vPwyZ8YAEmSJEmS1AI8Dqa0WL5+Kz99fjEfP+wAzpw8LFp84gkoLoQrRkVHwE76Pzjqy+ltVJIkSZKkDsIQSClXkwy59qk36JqVwS2fqb0NrLIS/ud/4DMjYFsJnHQrHP2VdLcqSZIkSVKH4XEwpdxv8ldQGEtwxzkTyO3dLVpcuhRqKmFCBoz8BBz91fQ2KUmSJElSB+NOIKXU0nVbue2FxRw/JpczJg2r+2LsWHjku1BdAcdenb4GJUmSJEnqoAyBlDLRMbASumVncssZ46JjYAAFBbB9C7z+c8g7BkZMT2+jkiRJkiR1QB4HU8r8+uXlFJeWc9e5Ezlg9zGw0lL4+MfhG8dDThmcend6m5QkSZIkqYNyJ5BSYum6Ldz+jyWccHgup04YWvfFDTdAAMsq4xTGJxCM/jhBwLs/gwenr2dJkiRJkjoSdwKp1VXXJLn6yTfo3iWTH9Y/BlZSAg89BN85g4O6vsiZLzwEBA1+7dq1qe9XkiRJkqSOyJ1AanX3v7yCkpXl3HTaOA7o1a3ui+uug759YFgZi9YfwtOLPp2+JiVJkiRJ6uAMgdSqlqzdws/+sYSTxg7m0+OH1H1RUQHr18N3zoENi7i14CpC/zlKkiRJktRq/H/dajXVNUmuebKEnt2yGh4DA+jdG157DQYsgz55PPrm2elrVJIkSZKkTsAQSK3mvv8u541Vm7nptLEM7Nm17os5c2DDBlj5CqyaAzOupDqZnb5GJUmSJEnqBBwMrVaxeM0W7nxxCScfMZhTxte7DayyEs4+G0aNgi/2gR4HwKQLyM1tegh0bm7qepYkSZIkqSNr1k6gIAhOCoJgcRAES4MguK6J738WBMG82p8lQRCUt3yrai921R4D690tm5tPG9fwy1mzIBaDKz8Ly/8F078G2TmsWQNh2PhnzZr0/DdIkiRJktTR7HcnUBAEmcAs4BPAKmBOEATPhGG4cHdNGIZX1av/OjCpFXpVO/HLfy/jzbLN/PxzkxlQ/xhYIgE//CGceCJUvgzd+sCRl6SvUUmSJEmSOpHm7ASaBiwNw3B5GIZVwGPAafuoPw/4Q0s0p/Zn0eoK7n7pbU4ZP4STjxjS8Msf/xjKy+F7X4W3/gpHfQW69kpPo5IkSZIkdTLNCYGGASvrva+qXWskCIIRwCjgpQ/emtqb3cfA+uRkc9Oex8DCEFavhs9/Htb/DbJ7RCGQJEmSJElKiZYeDH0u8FQYhjVNfRkEwZeALwHk5eW18B+tdPv5v5axIF7BLy+YTP8eXRp+GQTw+9/D+qXw82lw9Fehe//0NCpJkiRJUifUnJ1AZcCB9d6H16415Vz2cRQsDMNfhWE4NQzDqYMGDWp+l2rzFsQ3c89Lb3PqhKGcNG6PY2CLF8OiRdHn12ZBRiZMvyL1TUqSJEmS1Ik1JwSaA4wOgmBUEARdiIKeZ/YsCoLgMKAf8ErLtqi2rqo6yTVPvkHf7l3431PHNi648kr46EdhYykUPwwTPwe9hzSukyRJkiRJrWa/IVAYhtXAFcDzwCLgiTAMFwRBcFMQBKfWKz0XeCwMw7B1WlVbNetfS1m0uoJbzhhHvz2Pgb3wQvRz3XUw9z5IVsOMb6SnUUmSJEmSOrFmzQQKw/BZ4Nk91m7Y4/3GlmtL7cX8ss3M+tdSTp84lBPGDm74ZTIJ3/kOjBwJF50LP58C486C/qPS0qskSZIkSZ1ZSw+GVicSHQMroV+PLtzY1DGwRx+FefPgkUdg3oOwaxsc+62U9ylJkiRJkgyB9AHc+9LbvLVmC/d/fip9u3dpXLBmDRxzDJzxKbhrPBx2ChwwJvWNSpIkSZKkZg2GlhqZX7aZWf9exmcmDeMTh+c2XXTNNfDf/0LRg7CzHGa6C0iSJEmSpHQxBNJ7Vlldw9VPlDCgRxd+8OkmjoElEvDii9Hn5C6YfS986KMwfEoq25QkSZIkSfUYAuk9u+efS1m8dgu3nnkEfbpnNy645RY44QRYtgzmPQzb1sGx16S+UUmSJEmS9C5DIL0nb6wq5xf/WcaZk4fzscOaOAb2zjtw993whS/AyDwouAuGT4ORM1PeqyRJkiRJqmMIpGarrK7hmidLGNizCzd8+vCmi77/fcjIgJtugjefgvJSOPZqCILUNitJkiRJkhowBFKz3fXi2yxZu5VbzxxPn5wmjoEVF8PDD8M3vwnDhkH+HZA7Dg45MfXNSpIkSZKkBgyB1CzzVpbzy/8s45ypwznu0AOaLlq1CsaMgeuug7f+ChuWwLHfcheQJEmSJEltgCGQ9mvnrugYWG7vblx/yl6OgQF8+tMwfz707g0v3w79PwSHn566RiVJkiRJ0l4ZAmm/fvbiEpaui46B9e7WxDGwmhp47DGoro7mAS37J6yeBzOvgozM1DcsSZIkSZIaMQTSPhWVJrj/v8s598gD+cghg5oueuQROO88+Otfo/eX74Dew2D8ualrVJIkSZIk7ZMhkPZq564arn2yhMG9u/G9T43ZS9FOuP56mDoVTj0VYq9ArACOuRKyuqS2YUmSJEmStFdZ6W5Abdcd/1jCsvXb+P0l0+jV1DEwgHvugZUr4Xe/i46C5d8B3QfA5M+ntllJkiRJkrRP7gRSkwpjm7j/5eWcNy2PY0fv5RjYxo3wox/BySfDccfB6hJ4+wU4+nLo0j21DUuSJEmSpH0yBFIj0TGwNxjaJ2fvx8AAVq+GESPg//4ven/5DujaG468NDWNSpIkSZKkZvM4mBq57fnFLN+wjUcuPYqeXffxT2TcOJg3D4IANrwNC/8fHPstyOmbumYlSZIkSVKzuBNIDcx9ZxMPFKzgc0flMePggXsvfPxx2Lw5CoAA8u+ErG5w1FdT06gkSZIkSXpPDIH0rh1VNVzzZAnD+ubw3ZP3cQyssBDOPRfuvDN6Ly+FNx6DKV+AnnuZHyRJkiRJktLK42B610+fX8w7G7fz6GX7OAYWhvDtb8PAgfDNb0Zrs+8BAjjm6ynrVZIkSZIkvTeGQALg9RWb+O3sFXx++giOOWgfx8Cefx5eegnuvhv69IGt66DoIZhwLvQZnrqGJUmSJEnSe+JxMLG9qpprnypheL8cvnPSYXsvrKmJdgEddBB8+cvR2qs/h5oqmHlVapqVJEmSJEnvizuBxE/+vpjYxu384bKj6bGv28DKy2H4cLjoIujSBXYk4PVfw+Gnw4CDUtavJEmSJEl67wyBOrlXl2/kwdnvcNExI5l+0IB9Fw8YAM8+G80FgigAqtoSXQsvSZIkSZLaNI+DdWLbq6r59lNvMGJAd7590qH7Lv7LX2DFiuhzEEDVtugo2OgTYfARrd+sJEmSJEn6QAyBOrH/e+4tVia289OzJtC9yz42hW3YABdcAN+qt+On8HewYxN8+JrWb1SSJEmSJH1ghkCd1OxlG/jdKzEuOmYk00b133fxj34EW7fCD38YvVdXwuy7YeSxcOC01m9WkiRJkiR9YIZAndC2yugY2MgB3fn2ifu4DQxg+XKYNQsuvhjGjo3WSv4AW1Y7C0iSJEmSpHbEwdCd0I+fW0RZ+Q6e+PJ0crpk7rv4+ushKwv+93+j95pqyL8Thk6CDx3X+s1KkiRJkqQWYQjUycxeuoGHXy3lkpmjOHLkfo6BJZPQvz985zswdGi0tvDPkFgBJzwSDYiWJEmSJEntgiFQJ7K1spprn3qDDw3swTUn7Oc2MICMDLj33rr3ZBJevh0GHQaHntx6jUqSJEmSpBbnTKBO5JZnFxHfvIOfnj1+/8fACgqin/qW/B3WLYSZ34oCIkmSJEmS1G64E6iTyH97A4++Vsplx45iyoj9HAOrqYGvfAV27oS33oLMTAhDePk26DsCxp2ZmqYlSZIkSVKLMQTqJO777zKG98vh6uYcA3voIZg/H554IgqAAFb8F8oK4ZSfQab/bCRJkiRJam8809MJ1CRDimIJjjv0ALpl7+cY2I4d8P3vw7RpcNZZdesv3wY9B8OE81u3WUmSJEmS1Crc0tEJLF6zhW1VNUwZ0W//xXfdBWVl8Oijdbd/rZwT7QQ64YeQ3a11m5UkSZIkSa3CnUCdQGFpAqB5IVCvXnDhhfDhD9et5d8BOf1gyhdbqUNJkiRJktTaDIE6gaJYgkG9ujK8X87+i7/2tWgm0G5rF8DiZ+Gor0LXnq3XpCRJkiRJalWGQJ3A3NgmpuT1I9h9vKspy5dHR8CSyYbrL98BXXrCtMtat0lJkiRJktSqDIE6uHUVO1m5acf+j4L9z//AZZfBunV1axuXwYI/wdSLoft+rpWXJEmSJEltmiFQB1dUOw9o8r5CoNdfh8cfh6uvhsGD69YL7oKMbJj+tVbuUpIkSZIktTZDoA6uMJagS1YG44b1brogDOHb34ZBg+Daa+vWK+Iw71GYfCH0Gtz0r5UkSZIkSe2GV8R3cIWxBOOH9aFrVmbTBc8+C//5D8yaFd0MttvseyBMwjFXpqZRSZIkSZLUqtwJ1IHt3FXD/LKKfc8DysqCk06K5gHttm0DFD4I48+BfiNavU9JkiRJktT63AnUgS2Ib6aqJrnveUAnnhj91PfaL2HXDph5Ves2KEmSJEmSUsadQB1YYax2KHReEyHQ9u1w++3Rs76dFfDar2DMp2HQoSnoUpIkSZIkpYIhUAdWGEswYkB3BvXq2vjLO++Ea66B4uKG63N+DZWb4dhvpaZJSZIkSZKUEoZAHVQYhhTGypnS1C6g9evh1lvhtNNgxoy69art8MosOOjjMHRS6pqVJEmSJEmtzhCog1q5aQcbtlY2PQ/o5pujY2C33tpwvfhh2L4Bjr06NU1KkiRJkqSUMQTqoApLNwE0vhls6VL4xS/g0kvhsMPq1quroOAuyJsOI2cgSZIkSZI6FkOgDqowlqBn1ywOye3V8Itdu+D44+EHP2i4/uYTULHKXUCSJEmSJHVQXhHfQc19J8GkvL5kZgQNvxgzBp57ruFasgbyfwaDj4CDj09dk5IkSZIkKWXcCdQBbdm5i8VrtzS8Gj4M4Sc/gXi88S9Y9AxsXBrtAgqCxt9LkiRJkqR2zxCoA5q3spww3GMe0F//Ct/5DjzzTMPiMISXb4cBo2HMqaltVJIkSZIkpYwhUAdUGEsQBDApr2/d4r33wqhRcMklDYvf/geseRNmXgUZmaltVJIkSZIkpYwhUAdUGEtwaG4venXLjhaqq2H2bPjkJyE7u64wDOHl26DPgTD+nPQ0K0mSJEmSUsIQqIOpSYbMKy1veBTszTdh61aYscfV77HZsPI1OOZKyMxGkiRJkiR1XIZAHczb67awpbK6YQhUWgq9e8PMmQ2LX74degyCyRemtklJkiRJkpRyXhHfwRTGEsAeQ6FPOw02bYLMejN/yopg2T/h+BshOyelPUqSJEmSpNRzJ1AHUxhLMLBnF/L6d2/4ReYeQ5/z74CufWDqHoOiJUmSJElSh2QI1MEUxRJMzutHEATRQiwGRxwB//53XdG6t2DRX+CoL0G33mnpU5IkSZIkpZYhUAeyYWsl72zc3vAoWH4+zJ8PfetdF19wJ2R3h6O+mvomJUmSJElSWhgCdSBFTc0DKiiAXr2i3UAAiXfgjSdgyhehx4DUNylJkiRJktLCEKgDKSxNkJ0ZMG5Yn7rF/HyYPr1uJlDB3RBkwDFXpKdJSZIkSZKUFoZAHUjhOwnGDetDt+zawKe8PDoKtvtq+C1roPhhmHg+9B6avkYlSZIkSVLKGQJ1EJXVNbxRtpkpefWOgm3ZAuedB8cfH72/MguSu2DGN9LTpCRJkiRJSpusdDeglrEgXkFVdZKpI+uFQAceCI88En3evgnm/gbGnQkDDkpPk5IkSZIkKW3cCdRB7B4KPbn+TqA1axg8OCQI4IZP3g9VWzni8qsIAhg8OE2NSpIkSZKktHAnUAdRGEtwYP8cDujdLVqoqoJRo1i7cwcAnxr9PC/HpjN/3VgA1q5NV6eSJEmSJCkd3AnUAYRhyNxYouE8oKIi2LkTgOyMKiYOfpNXy6amqUNJkiRJkpRuzQqBgiA4KQiCxUEQLA2C4Lq91JwTBMHCIAgWBEHwaMu2qX1ZldjB+i2VTBlRLwQqKHj347gDFtI1q4q58Ulp6E6SJEmSJLUF+z0OFgRBJjAL+ASwCpgTBMEzYRgurFczGvguMCMMw0QQBAe0VsNqrKi0dh5Q/RAoPx8OOgiWwZHDigCYUzY5He1JkiRJkqQ2oDk7gaYBS8MwXB6GYRXwGHDaHjWXAbPCMEwAhGG4rmXb1L4UxhL06JLJobm9ooUwjHYCzZwJwNShxWzc3o8V5SPT16QkSZIkSUqr5gyGHgasrPe+Cjhqj5pDAIIgKAAygRvDMPx7i3So/SqMJZiY15eszNpMr6YG7rkHDjyQ3L/DkUOLa4+CBe/+mtzc9PQqSZIkSZLSo6UGQ2cBo4GPAucB9wdB0HfPoiAIvhQEwdwgCOauX7++hf7ozm1bZTWLVlc0HAqdlQWf/SwccwxrVu5g4pCFnPjFSYQh7/6sWZO+niVJkiRJUuo1JwQqAw6s9z68dq2+VcAzYRjuCsNwBbCEKBRqIAzDX4VhODUMw6mDBg16vz2rnpKV5STDPeYBvfgivPFG9HnNmxDWwFDnAUmSJEmS1Jk1JwSaA4wOgmBUEARdgHOBZ/ao+TPRLiCCIBhIdDxseQv2qb0ojEVDoSfV3wn0ta/B9ddHn8uiodAMMwSSJEmSJKkz228IFIZhNXAF8DywCHgiDMMFQRDcFATBqbVlzwMbgyBYCPwLuDYMw42t1bTqzI0lOCS3J31ysqOF9ethyRKYMSN6jxdDz1zoNSR9TUqSJEmSpLRrzmBowjB8Fnh2j7Ub6n0OgW/V/ihFksmQotIEp4wfWrdYUBA9a28GI14UHQULgsa/gSRJkiRJ6jRaajC00mDp+q1s2VnNlPrzgAoKoGtXmDoVdlbAhrdh6KT0NSlJkiRJktoEQ6B2bPc8oEYh0NSpURC0ugQInQckSZIkSZKadxxMbVNhLEH/Hl0YOaB73eLzz8PatdHneHH0dCeQJEmSJEmdniFQO1YUSzA5rx9B/Xk/vXpFPxDNA+qTBz0GpqdBSZIkSZLUZngcrJ3atK2K5Ru2NTwK9thjcNNNEIbRe7wYhk5MT4OSJEmSJKlNMQRqp4qamgf00EPw+OPRTWDbN0HiHecBSZIkSZIkwBCo3SosTZCVETB+eJ9oIZmE2bPrXQ3vPCBJkiRJklTHEKidKowlGDusD92yM6OFBQtg82aYMSN6jxdFzyEeB5MkSZIkSYZA7dKumiQlK8uZklfvKFh+fvR8dyfQPOh/EOT0TX2DkiRJkiSpzTEEaocWxiuorE42nAeUSMDBB8OoUdF7WZHzgCRJkiRJ0rsMgdqhuU0Nhf6f/4ElS6Kh0FvWwJa484AkSZIkSdK7DIHaoaJYgmF9cxjcp1vDL4Iger47FNqdQJIkSZIkKWII1M6EYchNWnpeAAAgAElEQVTc2KaGu4D++EeYMgVWrYre48UQZMCQ8elpUpIkSZIktTmGQO1MfPNO1lZUNgyB/vMfWLwYBg+O3suKYNBh0KVHepqUJEmSJEltjiFQO1PY1Dyg/Hw46ijIyoIwjHYCOQ9IkiRJkiTVYwjUzhTFEuRkZ3LY4F7RwpYtUFJSdzX85pWwfYMhkCRJkiRJasAQqJ0pjCWYeGBfsjJr/+pefRWSSZgxI3p3KLQkSZIkSWqCIVA7sr2qmoWrKxoeBevVC84+G44+OnovK4KMbBg8Lj1NSpIkSZKkNikr3Q2o+UpWbqYmGTYMgY4+Gp54ou49Xgy5YyGra+oblCRJkiRJbZY7gdqRotJoKPSkvL7RQnU1lJXVFSSTEJ/nPCBJkiRJktSIIVA7UhhLcPABPenbvUu0UFICw4fD009H74kVULkZhjkPSJIkSZIkNWQI1E4kkyFFpQmm5O1xNTzAkUdGz7Ki6OlOIEmSJEmStAdDoHZi+YZtlG/f1XAeUH4+jBgR7QYCiBdBVjcYNCY9TUqSJEmSpDbLEKidKIxtAmDKyNoQKAyhoABmzqwrihfD4PGQ6bxvSZIkSZLUkCFQO1EYS9C3ezYfGtgjWlixAlavhhkzoveaalhd4jwgSZIkSZLUJLeMtBOFsWgeUBAE0cKAAfDQQ/CRj0TvG5bAru3OA5IkSZIkSU1yJ1A7kNhWxbL125hcfx5Qnz5w4YWQlxe9x3cPhXYnkCRJkiRJaswQqB0oXpkAaDgU+tFHYdmyuvd4MXTpBQMOTnF3kiRJkiSpPTAEagcKYwkyMwImDO8bLWzaBJ/7HDz+eF1RWREMnQgZ/pVKkiRJkqTGTAzagcJYgrFDe5PTJTNamD07eu4eCl1dBWvnRyGQJEmSJElSEwyB2rhdNUlKVm5mcl69o2AFBZCdDUceGb2vWwA1Vc4DkiRJkiRJe2UI1Ma9tXoLO3bVNJwHlJ8PkydD9+7Re7w4enozmCRJkiRJ2gtDoDauMLYJqDcUetcumDsXZs6sKyorgpx+0G9k6huUJEmSJEntQla6G9C+FZaWM6RPN4b2zYkWsrMhHoeqqrqi+LxoF1AQpKdJSZIkSZLU5rkTqI0riiWYXP8oGEC/fpCbG32u2g7rFjoPSJIkSZIk7ZMhUBu2evMOysp3MLV+CHTzzXD//XXva+dDWOM8IEmSJEmStE+GQG1YYSwB1JsHFIZw113wyit1RWVF0XOYO4EkSZIkSdLeGQK1YYWxBN2yMxgzpHe0sHgxbNwIM2bUFcWLoWcu9BqSniYlSZIkSVK7YAjUhhXFEkwY3pfszNq/pvz86Fn/ZrB4UTQPyKHQkiRJkiRpHwyB2qgdVTUsiFfUHQUDKCiAgQPhkEOi950VsOFtj4JJkiRJkqT9MgRqo95YVU51MmwYAlVXw/HH1+36WV0ChA6FliRJkiRJ+5WV7gbUtMLSaCj0pLx6IdDvfx8Nh94tXhw9DYEkSZIkSdJ+uBOojSqKJfjQoB7079Gl4Rf1Z//Ei6BPHvQYmNrmJEmSJElSu2MI1AaFYUhhLMGU+ruArrsOjjuu8U6gYe4CkiRJkiRJ+2cI1Aat2LCNxPZdDecBvfQSJJN1O4G2b4LEOx4FkyRJkiRJzWII1AYVxqJ5QO+GQNu2QXHxHlfD754H5M1gkiRJkiRp/wyB2qCi0gS9u2Vx0KCe0cLrr0c3g82YUVcUL4qeQyakvkFJkiRJktTuGAK1QYWxBFNG9CMjo/boV35+dAxs+vS6orJiGHAw5PRNT5OSJEmSJKldMQRqYzbv2MWStVsbzgM6/HC44groV28tXuw8IEmSJEmS1GxZ6W5ADRWVRvOAJtcPgc48M/rZbcsa2BJ3HpAkSZIkSWo2dwK1MUWxBJkZAROG1x7zKi+HdesaFr07FNqdQJIkSZIkqXkMgdqYwliCMUN60aNr7SatRx6B3FxYubKuqKwIggwYMj49TUqSJEmSpHbHEKgNqa5JMm9lOVPy6h0FKyiAYcNg+PC6tXgxDDoMuvRIfZOSJEmSJKldMgRqQ95as4XtVTUN5wHl50dXwwe1N4WFYXQ9vPOAJEmSJEnSe2AI1IbsHgr97s1gpaXRMbCZM+uKNq+E7Rth6MQ0dChJkiRJktorQ6A2pDCWILd3V4b1zYkWCgqi54wZdUVlRdFzmDuBJEmSJElS8xkCtSGFsQRTRvQj2H306yMfgQcegPH1BkDHiyEjG3LHpadJSZIkSZLULhkCtRFrK3ayKrGDyfWHQg8dChdfDFlZdWvxIsgdC1ldU9+kJEmSJElqtwyB2oii2B7zgCoq4Ne/hjVr6oqSSYiXwNBJaehQkiRJkiS1Z4ZAbURhLEHXrAzGDu0TLbzyClx2GSxYUFe0aTlUbnYekCRJkiRJes8MgdqIwtIEE4b3pUtW7V9Jfj5kZsJRR9UVxYujp9fDS5IkSZKk98gQqA3YuauG+WWbmTyi3jyg/HyYOBF69qxbixdBVg4MOiz1TUqSJEmSpHbNEKgNeLNsM7tqwrp5QLt2wWuvwcyZDQvjxTBkPGRmNf5NJEmSJEmS9sEQqA0orB0KPTmvb7SwYAHs2AEzZtQV1VTDaodCS5IkSZKk98ctJW1AYSzBqIE9GNCz9tr3iRNh7Vro0aOuaMMS2LXdeUCSJEmSJOl9cSdQmoVhSFEsweS8fg2/OOCAhiFQvCh6uhNIkiRJkiS9D4ZAaRbbuJ2N26rq5gGFIVx4ITz7bMPCeDF06QUDDk59k5IkSZIkqd0zBEqz3fOA3g2Bli2Dhx+GVasaFpYVwdCJkOFfmSRJkiRJeu9MFNKssDRBr65ZjD6g9ir4/PzoWX8odHUVrJ3vUTBJkiRJkvS+GQKlWVEswaQR/cjICKKFggLo1w/GjKkrWrcAaqoMgSRJkiRJ0vvWrBAoCIKTgiBYHATB0iAIrmvi+4uCIFgfBMG82p9LW77Vjqdi5y4Wr93ClPpDofPz4ZhjGh77ihdHz2HeDCZJkiRJkt6f/V4RHwRBJjAL+ASwCpgTBMEzYRgu3KP08TAMr2iFHjuseaXlhCFMHVkbAlVWRruAjjuuYWFZEeT0h74jUt+kJEmSJEnqEPYbAgHTgKVhGC4HCILgMeA0YM8QSO9RYSxBRgATDuwbLXTtCrNnNy6MF0dHwYIgtQ1KkiRJkqQOoznHwYYBK+u9r6pd29OZQRC8EQTBU0EQHNgi3XVwRaUJDhvcm55da7O4MGxcVLUd1i1yHpAkSZIkSfpAWmow9F+AkWEYjgf+AfyuqaIgCL4UBMHcIAjmrl+/voX+6PapJhlSXFpedzU8wPHHw9e+1rBwzZsQ1jgPSJIkSZIkfSDNCYHKgPo7e4bXrr0rDMONYRhW1r7+GpjS1G8UhuGvwjCcGobh1EGDBr2ffjuMxWu2sLWyui4E2rkzGgrdo0fDwt1Dod0JJEmSJEmSPoDmhEBzgNFBEIwKgqALcC7wTP2CIAiG1Hs9FVjUci12TIWlCYC6EGjuXKiqghkzGhbGi6DnYOg9NMUdSpIkSZKkjmS/g6HDMKwOguAK4HkgE/hNGIYLgiC4CZgbhuEzwJVBEJwKVAObgItasecOoSiWYFCvrgzvlxMt5OdHz2OOaVi4eyi0JEmSJEnSB9Cc28EIw/BZ4Nk91m6o9/m7wHdbtrWOrTCWYEpeP4LdN34VFMBhh0H9Y3I7K2DD23DE2elpUpIkSZIkdRgtNRha78G6LTsp3bS94VDoE0+Eyy9vWLi6BAjdCSRJkiRJkj6wZu0EUssqipUDMLl+CHTFFY0L40XRc6g3g0mSJEmSpA/GnUBpUFSaoEtmBuOG9Y4WVq6ETZsaF8aLoW8e9BiQ2gYlSZIkSVKHYwiUBoWxBEcM70PXrMxo4Xvfg7FjIQwbFpYVeRRMkiRJkiS1CEOgFKusruHNVZuZWv8oWH5+dCvY7iHRANs3QXnMo2CSJEmSJKlFGAKl2PyyCqpqknXzgOJxWLECZs5sWPjuPCB3AkmSJEmSpA/OECjFimIJACbn1YZABQXRc8aMhoXx4ug5dGKKOpMkSZIkSR2ZIVCKFcYSjBjQnUG9ukYLBQWQkwOT9tjxU1YMAw6Gbn1S36QkSZIkSepwvCI+hcIwZG4swYdHD6xbvPxy+OhHITu7YXG8GEbucURMkiRJkiTpfTIESqGVm3awYWtl3TwggEMOiX7q27IGtsSdByRJkiRJklqMx8FSqLB0EwBTdodACxfCQw/Btm0NC3fPAxrmzWCSJEmSJKllGAKlUGEsQc+uWRyS2ytaePJJuOgiqK5uWFhWBEEGDD4i5T1KkiRJkqSOyRAohQpj5UzK60tmRhAtFBTAEUdAnz2GP8eLYdAY6NIj9U1KkiRJkqQOyRAoRbbs3MXiNRV1V8NXV8Mrr8DMPYY/hyHEi5wHJEmSJEmSWpQhUIqUrNxMMqw3D+jNN2Hr1sYh0OaVsH0jDDMEkiRJkiRJLccQKEUKYwmCACbm9Y0Wioqi54wZDQvLatfdCSRJkiRJklqQIVCKFJYmODS3F727ZUcLl1wC8Tjk5TUsjBdBRjbkjkt9k5IkSZIkqcMyBEqBZDKkOJaoOwq225AhjYvjxZA7FrK6pqY5SZIkSZLUKRgCpcDb67aypbK6LgSKxeCss6CkpGFhMgnxeTBscuqblCRJkiRJHZohUAoUxhJAvaHQL78Mf/xj48JNy6GywnlAkiRJkiSpxRkCpUBhLMHAnl3I6989WsjPh969Ydwec3/iu4dCuxNIkiRJkiS1LEOgFCiMbWJyXj+CIIgWCgpg+nTIzGxYGC+GrBwYdFjqm5QkSZIkSR2aIVAr27C1knc2bq87CpZIwPz5MHNm4+KyIhgyHjKzUtukJEmSJEnq8AyBWlnRnvOA1qyBKVPgwx9uWFhTDWvecB6QJEmSJElqFW45aWWFpQmyMwPGDesTLYwZA3PnNi7csBh2bXcekCRJkiRJahXuBGplRbEE44b1oVt27fyfZLLpwnhx9PR6eEmSJEmS1AoMgVpRVXWSklWbmZJXexSsqgoOOABmzWpcXFYEXXtD/4NS26QkSZIkSeoUDIFa0YL4Zqqqk3XzgIqKYONGGDKkcXG8GIZMgAz/SiRJkiRJUsszcWhFhXsOhc7Pj54zZjQsrK6CtfMdCi1JkiRJklqNIVArKipNcGD/HA7o3S1aKCiAgw+G3NyGhesWQE2V84AkSZIkSVKrMQRqJWEYUhhL1M0DCsNoJ9Ceu4AgmgcE7gSSJEmSJEmtxiviW0lZ+Q7WVlTWHQWrrISvfx2OPLJxcbwYcvpD3xGpbVKSJEmSJHUahkCtZPc8oMm7Q6Bu3eCGG5oujhdHu4CCIEXdSZIkSZKkzsbjYK2kKJagR5dMDs3tFS0sXAgVFY0Lq7bDukXOA5IkSZIkSa3KEKiVzI0lmJjXl6zM2v+JzzgDLrigceGaNyGscR6QJEmSJElqVYZArWBbZTWLVlfUDYVetw6WLIGZMxsXx4uj51B3AkmSJEmSpNZjCNQKSlaWkwzrzQMqKIieTYZARdBzMPQekroGJUmSJElSp2MI1Ap2D4WelFcvBOraFaZMaVwcL3YekCRJkiRJanWGQK2gsDTBIbk96ZOTHS3k58O0aVEQVN/OCtjwtvOAJEmSJElSq/OK+BaWTIYUxRJ8any941333QfbtzcuXl0ChM4DkiRJkiRJrc4QqIUtW7+Vip3VTBnRv25xwoSmi+NF0dOdQJIkSZIkqZV5HKyF7Z4HNGX3UOjnn4fHH2+6uKwI+uZBjwEp6k6SJEmSJHVW7gRqYYWxBP17dGHkgO7Rwl13QWkpfPazjYvjxe4CkiRJkiRJKeFOoBZWWJpgcl4/giCAZBJmz4YZMxoXbtsI5THnAUmSJEmSpJQwBGpBm7ZVsXz9trqjYAsWwObNMHNm4+LVxdHTnUCSJEmSJCkFDIFaUHHpHvOA8vOjZ1M7gcp2h0ATU9CZJEmSJEnq7AyBWlBhLEFWRsD44X2ihfnzYcgQGDWqcXG8GAaMhm59UtukJEmSJEnqlAyBWtDcWIKxw/rQLTszWrj3XnjzTQiCxsXxIo+CSZIkSZKklDEEaiG7apKUrCxnSl6/usUggAFNXP9esRq2rIZhDoWWJEmSJEmpYQjUQhbGK6isTtbNA3r2WTj/fNi4sXFx3KHQkiRJkiQptQyBWkhhLBoKPXlE32jhuefgmWegTxMzf+LFEGTA4PEp7FCSJEmSJHVmhkAtpLA0wbC+OQzpkxMtFBTAUUdBVlbj4ngRDBoDXbqntklJkiRJktRpGQK1kKJYou4o2JYtUFICM2c2LgzDaCfQMI+CSZIkSZKk1DEEagHx8h2s3ryzLgR69VVIJpsOgcpLYftG5wFJkiRJkqSUMgRqAbvnAb0bAlVWwoQJcPTRjYvfHQrtzWCSJEmSJCl1DIFaQGEsQU52JocN7hUtnHIKzJsHvXo1Lo4XQUY25I5NbZOSJEmSJKlTMwRqAUWlCSYe2JeszIzoGFhNzd6L48UweBxkdU1dg5IkSZIkqdMzBPqAtldVsyBeUXcUrKgI+veH//yncXEyCfF5zgOSJEmSJEkpZwj0Ab2xajM1ybAuBMrPh4oKOPjgxsWblkNlhfOAJEmSJElSyhkCfUC7h0JPyusbLeTnw8iRMGxY4+J4UfR0J5AkSZIkSUoxQ6APqDCW4OADetK3excIQygogBkzmi6OF0NWDgw6LLVNSpIkSZKkTs8Q6ANIJkOKShNMyas9CrZ8OaxZAzNnNv0LyopgyHjIzEpdk5IkSZIkSYBpxAdQE4Zc/6nDGTmge7TQrRt873tw/PFNFP//9u49uqrqXNj4MxPCRaCiqIDgBRzUBhIIJWDkokELptojIn7HY63llNa2UpR2tFXrEW9V663UqnjaajW9+CGtGEtbW4VaioC1hBDkpiB+UUIEBRoqlwiB+f2RmIKEkIQddiDPb4yMnaw511zv3nvMseR1zndVwvrX4NPjDm+QkiRJkiRJmAQ6JGmpKVw2sMe/D3TvDnfeWXvnjW/Aru3WA5IkSZIkSUnhdrBEeuUV2Lat9rayxVWv3X0ymCRJkiRJOvxMAiXK5s0wZAj8+Me1t68rgjafgOPPOLxxSZIkSZIkYRIocRYsqHo9UFHosiLo1h9S/MglSZIkSdLhZ0YiUebNg7Q0GDRo/7bKD2H9MreCSZIkSZKkpDEJlCjz58PAgdCu3f5tG5bDnl0WhZYkSZIkSUlTr6eDhRDygB8DqcDjMcZ7DtBvLPAMMCjGWJiwKJu7igr4xz/g2mtrb/+oKPTJrgSSJEmSJB0+u3btorS0lIqKimSHogRr27YtPXr0IC0trd7nHDQJFEJIBaYCI4FSYGEIYWaMccXH+nUEJgGvNijqo0FaGrz8Mhx3XO3tZUXQ7njodOrhjUuSJEmS1KKVlpbSsWNHTj/9dEIIyQ5HCRJjZNOmTZSWltKzZ896n1ef7WCDgTdjjG/FGHcCTwOja+n3feBeoOWlF1NTYfBg6N279vay4qp6QE44SZIkSdJhVFFRQefOnU0AHWVCCHTu3LnBK7zqkwTqDqzd6+/S6mN7X/zTwCkxxj826OpHi8cfh1mzam/buR3eW2k9IEmSJElSUpgAOjo15ns95MLQIYQUYArw7Xr0/WoIoTCEUPj+++8f6qWbhz174MYbYdq02tvXL4W423pAkiRJkiQpqeqTBFoHnLLX3z2qj32kI5ABzAkhlAA5wMwQQvbHB4ox/izGmB1jzD7xxBMbH3Vz8sYbsGkTDBtWe3tZUdWrK4EkSZIkSS1QSUkJGRkZ9epbWFjIdddd16Dxv/KVr7BixYqDd6ynsrIyLrvssoP2u/DCCykvL6e8vJxHH300YddvSvVJAi0EeocQeoYQWgP/Bcz8qDHGuCXGeEKM8fQY4+nA34GLW8zTwebNq3o9YBJoMXTsBp/odvhikiRJkiTpCJSdnc1DDz3UoHMef/xx+vTpk7AYTj75ZJ555pmD9nv++efp1KnT0ZUEijFWAhOBF4CVwG9ijMtDCHeEEC5u6gCbvfnz4cQTD1wUel2Rq4AkSZIkSQLeeustBgwYwKuvvsrZZ5/NgAEDGDJkCG+88QYAc+bM4XOf+xwAt912G+PGjWP48OGcdtppPPvss1x//fVkZmaSl5fHrl27AMjNzaWwsGodSocOHfif//kf+vfvT05ODhs2bABgzZo15OTkkJmZyc0330yHDh0OGOPeK5fy8/O59NJLycvLo3fv3lx//fU1/U4//XQ2btzIjTfeyJo1a8jKyuK73/1u4j+0BKpXTaAY4/Mxxk/GGM+IMd5VfeyWGOPMWvrmtphVQFC1HWzYsNqf/FXxL9i02npAkiRJkqTmITd3/5+PVrFs3157e35+VfvGjfu3NcAbb7zB2LFjyc/PJz09nZdffpnFixdzxx13cNNNN9V6zpo1a3jppZeYOXMmX/jCFxgxYgRLly6lXbt2/PGP+z+batu2beTk5LBkyRLOOeccHnvsMQAmTZrEpEmTWLp0KT169GhQ3MXFxUyfPp2lS5cyffp01q5du0/7PffcwxlnnEFxcTH3339/g8Y+3FolO4Aj3oIFsHVr7W3vFle9uhJIkiRJktSCvf/++4wePZpnn32WPn36sHbtWsaNG8fq1asJIdSs6vm4z372s6SlpZGZmcnu3bvJy8sDIDMzk5KSkv36t27dumYl0cCBA5lV/STvV155heeeew6Az3/+83znO9+pd+znn38+xx57LAB9+vTh7bff5pRTTjnIWc2TSaBDFQJ07Fh7W9niqleTQJIkSZKk5mDOnAO3HXNM3e0nnFB3ex2OPfZYTj31VObNm0efPn2YPHkyI0aMoKCggJKSEnIPsKqoTZs2AKSkpJCWllbzWPSUlBQqKyv36793n9TU1Fr7NNRHMSRyzGQxCdSU1hVBp1OhfedkRyJJkiRJUtK0bt2agoICLrjgAjp06MCWLVvo3r07UFV3p6nl5OQwY8YMLr/8cp5++umEjt2xY0c++OCDhI7ZVOpVE0iNVLbYekCSJEmSJAHt27fnD3/4Az/60Y/Iysrie9/7HgMGDDgsK2sefPBBpkyZQr9+/XjzzTdrtnclQufOnRk6dCgZGRnNvjB0iDEm5cLZ2dnxo+rdR6Vtm+D+XvCZ22HYN5MdjSRJkiSpBVq5ciXp6enJDiPptm/fTrt27Qgh8PTTTzNt2jR+97vfJTusQ1bb9xtCWBRjzK6tv9vBmsq71fWAursSSJIkSZKkZFq0aBETJ04kxkinTp144oknkh1SUpgEairrqpNA3fonNw5JkiRJklq44cOHs2TJkn2OLV26lKuuumqfY23atOHVV189nKEdViaBmkrZYujcG9ombp+hJEmSJElKjMzMTIqLi5MdxmFlYeimUlbko+ElSZIkSVKzYRKoKfzrXfjgXesBSZIkSZKkZsMkUFMoq64H5OPhJUmSJElSM2ESqCmULYaQCl0zkx2JJEmSJEkSYBKoaZQVwUnp0PqYZEciSZIkSVKLdvrpp7Nx48YDtg8ZMuSgY3zlK19hxYoViQwrKUwCJVqMsK4ITs5KdiSSJEmSJNVb164Qwv4/XbsmL6bKysomv8aCBQsO2ufxxx+nT58+TR5LUzMJlGjl78COzdYDkiRJkiQdUTZsaNjx+iopKSE9PZ2rr76avn37MmrUKHbs2EFxcTE5OTn069ePMWPG8M9//hOA3NxcvvnNb5Kdnc2Pf/xjcnNz+da3vkV2djbp6eksXLiQSy+9lN69e3PzzTfXXOeSSy5h4MCB9O3bl5/97Gf1jq9Dhw4AzJkzh9zcXC677DI+9alPceWVVxJjrImpsLDw0D6IZsAkUKKVFVW9+nh4SZIkSZIAWL16Nd/4xjdYvnw5nTp1YsaMGXzxi1/k3nvv5bXXXiMzM5Pbb7+9pv/OnTspLCzk29/+NgCtW7emsLCQr3/964wePZqpU6eybNky8vPz2bRpEwBPPPEEixYtorCwkIceeqjmeEMsXryYBx98kBUrVvDWW28xf/78xHwAzYRJoEQrWwypraFL32RHIkmSJElSs9CzZ0+ysqrKpgwcOJA1a9ZQXl7OueeeC8C4ceOYO3duTf/LL798n/MvvvhiADIzM+nbty/dunWjTZs29OrVi7Vr1wLw0EMP0b9/f3Jycli7di2rV69ucJyDBw+mR48epKSkkJWVRUlJSWPebrPVKtkBHHXWFVUlgFq1SXYkkiRJkiQ1C23a/PvfyKmpqZSXl9fZv3379rWen5KSss9YKSkpVFZWMmfOHGbPns0rr7zCMcccQ25uLhUVFYcc5+GoSXQ4uRIokfbsgXeXWA9IkiRJkqQ6HHvssRx33HG8/PLLAPzqV7+qWRXUGFu2bOG4447jmGOO4fXXX+fvf/97okI9qrgSKJE2r4EP/2U9IEmSJEnSEadLl9qLQHfp0jTX+8UvfsHXv/51tm/fTq9evXjyyScbPVZeXh4/+clPSE9P58wzzyQnJyeBkR49wkeVrg+37OzseDRU1t7Ha7+BZ6+GaxZYE0iSJEmSlHQrV64kPT092WGoidT2/YYQFsUYs2vr73awRFpXBK3awQlnJjsSSZIkSZKkfbgdLJHKFkO3/pDqxypJkiRJUnOxadMmzj///P2O/+Uvf6Fz585JiCg5zFYkyu7KqqLQA/872ZFIkiRJkqS9dO7cmeLi4mSHkXRuB0uUjW9A5Q7o7pPBJEmSJElS82MSqJG6doUQ/v0zPq8IgKH/xyeDSZIkSZKk5sckUCN9/LF52ScvZkvFJ3hl1RnJCUiSJEmSJKkOJoESZNDJRSx6tz/Rj1SSJEmSJDVDZiwSoHXqh/TvuoyFZdYDkiRJkiRpb6mpqWRlZdX8lJSUMGTIkAaPk5+fz8SJExt0zm9/+1vS09MZMWJEg0lIPB8AAA6aSURBVK9XH3PmzOFzn/tcg88rLy/n0UcfbYKI6ubTwRIg86TltE7dRWGZ9YAkSZIkSdpbu3bt9nsy14IFCw7LtX/+85/z2GOPMWzYsHr1r6yspFWrpk+VfJQEmjBhQpNfa28mgRIg++TFACxc50ogSZIkSVLzdPvvl7Oi7F8JHbPPyZ/g1v/o2+DzOnTowNatWykoKOCRRx5h9uzZrF+/nnPPPZe5c+fStWvXWs9bu3Ytubm5rFu3ji984QvceuutAPz617/moYceYufOnZx11lk8+uij3HXXXcybN48vf/nLXHzxxXz/+9/nmmuuobCwkFatWjFlyhRGjBhBfn4+zz77LFu3bmX37t08//zzXHvttSxbtoxdu3Zx2223MXr06IO+p3/84x9MmjSJiooK2rVrx5NPPsmZZ57J8uXL+dKXvsTOnTvZs2cPM2bMYPLkyaxZs4asrCxGjhzJ/fff3+DPsDFMAjVSly7/Lg49qHsR72/rzNtbTqVLl+TGJUmSJElSc7Jjxw6ysrIA6NmzJwUFBTVtY8aMYcaMGUydOpU///nP3H777QdMAEFVomXZsmUcc8wxDBo0iIsuuoj27dszffp05s+fT1paGhMmTOCpp57illtu4aWXXuKBBx4gOzubH/7wh4QQWLp0Ka+//jqjRo1i1apVABQVFfHaa69x/PHHc9NNN3HeeefxxBNPUF5ezuDBg/nMZz5D+/bt63yfn/rUp3j55Zdp1aoVs2fP5qabbmLGjBn85Cc/YdKkSVx55ZXs3LmT3bt3c88997Bs2bL9Vkg1NZNAjbR+/V5//G8xdBxAjCFp8UiSJEmSVJfGrNhJhNq2g+3t4YcfJiMjg5ycHK644oo6xxo5ciSdO3cG4NJLL2XevHm0atWKRYsWMWjQIKAq6XTSSSftd+68efO49tprgaqEzWmnnVaTBBo5ciTHH388AC+++CIzZ87kgQceAKCiooJ33nmH9PT0OmPbsmUL48aNY/Xq1YQQ2LVrFwBnn302d911F6WlpVx66aX07t27znGakkmgQ7VzO7y3Es68MNmRSJIkSZJ0xCktLSUlJYUNGzawZ88eUlIO/AyrEMJ+f8cYGTduHD/4wQ8aHcPeq3xijMyYMYMzzzyzQWNMnjyZESNGUFBQQElJCbm5uQB8/vOf56yzzuKPf/wjF154IT/96U/p1atXo2M9FD4d7FCtXwpxN5xsUWhJkiRJkhqisrKS8ePHM23aNNLT05kyZUqd/WfNmsXmzZvZsWMHzz33HEOHDuX888/nmWee4b333gNg8+bNvP322/udO3z4cJ566ikAVq1axTvvvFNroueCCy7g4YcfJsYIwOLFi+v1XrZs2UL37t2BqieZfeStt96iV69eXHfddYwePZrXXnuNjh078sEHH9Rr3EQyCXSoyoqqXk0CSZIkSZLUIHfffTfDhw9n2LBhTJkyhccff5yVK1cesP/gwYMZO3Ys/fr1Y+zYsWRnZ9OnTx/uvPNORo0aRb9+/Rg5ciTvvvvufudOmDCBPXv2kJmZyeWXX05+fj5t2rTZr9/kyZPZtWsX/fr1o2/fvkyePLle7+X666/ne9/7HgMGDKCysrLm+G9+8xsyMjLIyspi2bJlfPGLX6Rz584MHTqUjIwMvvvd79Zr/EQIH2W2Drfs7OxYWFiYlGsn1LNfhf83F779erIjkSRJkiRpHytXrjxoLRsduWr7fkMIi2KM2bX1dyXQoVpXBCf7aHhJkiRJktS8WRj6UOzeBW06wimDkx2JJEmSJElHhRdeeIEbbrhhn2Mff7T84dYcY2oMt4NJkiRJknSUcjvY0c3tYJIkSZIkSdqPSSBJkiRJkqQWwCSQJEmSJElSC2ASSJIkSZIkqQUwCSRJkiRJkppMamoqWVlZNT8lJSUMGTKkwePk5+czceLEhMdXUlJCRkZGwsdNlETG5yPiJUmSJElSk2nXrh3FxcX7HFuwYEGSojn8du/eTWpqarLDAEwCSZIkSZLUMvzpRli/NLFjds2Ez97T4NM6dOjA1q1bKSgo4JFHHmH27NmsX7+ec889l7lz59K1a9dazysrKyMvL481a9YwZswY7rvvPgBefPFFbr31Vj788EPOOOMMnnzySTp06MAdd9zB73//e3bs2MGQIUP46U9/SgiBRYsWMX78eABGjRpVZ6z5+fnMnDmT7du373fdadOmcffddxNj5KKLLuLee++teX9f+9rXmD17NlOnTiUvL49rrrmG559/nm7dunH33Xdz/fXX88477/Dggw9y8cUXU1JSwlVXXcW2bdsAeOSRRxq1YqoubgeTJEmSJElNZseOHTVbwcaMGbNP25gxY+jWrRtTp07l6quv5vbbbz9gAgiguLiY6dOns3TpUqZPn87atWvZuHEjd955J7Nnz6aoqIjs7GymTJkCwMSJE1m4cCHLli1jx44d/OEPfwDgS1/6Eg8//DBLliyp13uo7bplZWXccMMNvPTSSxQXF7Nw4UKee+45ALZt28ZZZ53FkiVLGDZsGNu2beO8885j+fLldOzYkZtvvplZs2ZRUFDALbfcAsBJJ53ErFmzKCoqYvr06Vx33XUN/qwPxpVAkiRJkiS1BI1YsZMItW0H29vDDz9MRkYGOTk5XHHFFXWOdf7553PssccC0KdPH95++23Ky8tZsWIFQ4cOBWDnzp2cffbZAPz1r3/lvvvuY/v27WzevJm+ffsyfPhwysvLOeeccwC46qqr+NOf/tTg627atInc3FxOPPFEAK688krmzp3LJZdcQmpqKmPHjq05v3Xr1uTl5QGQmZlJmzZtSEtLIzMzk5KSEgB27drFxIkTKS4uJjU1lVWrVtUZU2OYBJIkSZIkSUlTWlpKSkoKGzZsYM+ePaSkHHjTUps2bWp+T01NpbKykhgjI0eOZNq0afv0raioYMKECRQWFnLKKadw2223UVFR0agYa7tuXdq2bbtPHaC0tDRCCACkpKTUjJeSklIz1o9+9CO6dOnCkiVL2LNnD23btm1UrHVxO5gkSZIkSUqKyspKxo8fz7Rp00hPT6/ZxtUQOTk5zJ8/nzfffBOo2oq1atWqmoTPCSecwNatW3nmmWcA6NSpE506dWLevHkAPPXUU42KffDgwfztb39j48aN7N69m2nTpnHuuec2aiyALVu20K1bN1JSUvjVr37F7t27Gz3WgbgSSJIkSZIkJcXdd9/N8OHDGTZsGP3792fQoEFcdNFFpKen13uME088kfz8fK644go+/PBDAO68804++clPcvXVV5ORkUHXrl0ZNGhQzTlPPvkk48ePJ4Rw0MLQB9KtWzfuueceRowYUVMYevTo0Y0aC2DChAmMHTuWX/7yl+Tl5dG+fftGj3UgIcaY8EHrIzs7OxYWFibl2pIkSZIktQQrV65sUEJFR5bavt8QwqIYY3Zt/d0OJkmSJEmS1AK4HUySJEmSJDUbL7zwAjfccMM+x3r27ElBQcFRed3DySSQJEmSJElqNi644AIuuOCCFnPdw8ntYJIkSZIkHcWSVQtYTasx36tJIEmSJEmSjlJt27Zl06ZNJoKOMjFGNm3aRNu2bRt0ntvBJEmSJEk6SvXo0YPS0lLef//9ZIeiBGvbti09evRo0DkmgSRJkiRJOkqlpaXRs2fPZIehZsLtYJIkSZIkSS2ASSBJkiRJkqQWwCSQJEmSJElSCxCSVSE8hPA+8HZSLp54JwAbkx2EdBRwLkmJ43ySEsO5JCWGc0lKnIPNp9NijCfW1pC0JNDRJIRQGGPMTnYc0pHOuSQljvNJSgznkpQYziUpcQ5lPrkdTJIkSZIkqQUwCSRJkiRJktQCmARKjJ8lOwDpKOFckhLH+SQlhnNJSgznkpQ4jZ5P1gSSJEmSJElqAVwJJEmSJEmS1AKYBDoEIYS8EMIbIYQ3Qwg3Jjse6UgWQigJISwNIRSHEAqTHY90pAghPBFCeC+EsGyvY8eHEGaFEFZXvx6XzBilI8UB5tNtIYR11fen4hDChcmMUToShBBOCSH8NYSwIoSwPIQwqfq49yepAeqYS42+N7kdrJFCCKnAKmAkUAosBK6IMa5IamDSESqEUAJkxxg3JjsW6UgSQjgH2Ar8MsaYUX3sPmBzjPGe6v9JcVyM8YZkxikdCQ4wn24DtsYYH0hmbNKRJITQDegWYywKIXQEFgGXAP+N9yep3uqYS/9JI+9NrgRqvMHAmzHGt2KMO4GngdFJjkmS1MLEGOcCmz92eDTwi+rff0HVfyxIOogDzCdJDRRjfDfGWFT9+wfASqA73p+kBqljLjWaSaDG6w6s3evvUg7xy5BauAi8GEJYFEL4arKDkY5wXWKM71b/vh7oksxgpKPAxBDCa9Xbxdy+IjVACOF0YADwKt6fpEb72FyCRt6bTAJJai6GxRg/DXwW+Eb1knxJhyhW7ft277fUeP8LnAFkAe8CP0xuONKRI4TQAZgBfDPG+K+927w/SfVXy1xq9L3JJFDjrQNO2evvHtXHJDVCjHFd9et7QAFVWy4lNc6G6j3kH+0lfy/J8UhHrBjjhhjj7hjjHuAxvD9J9RJCSKPqH61PxRifrT7s/UlqoNrm0qHcm0wCNd5CoHcIoWcIoTXwX8DMJMckHZFCCO2rC50RQmgPjAKW1X2WpDrMBMZV/z4O+F0SY5GOaB/9g7XaGLw/SQcVQgjAz4GVMcYpezV5f5Ia4EBz6VDuTT4d7BBUP4btQSAVeCLGeFeSQ5KOSCGEXlSt/gFoBfxf55NUPyGEaUAucAKwAbgVeA74DXAq8DbwnzFGi91KB3GA+ZRL1XL7CJQAX9urpomkWoQQhgEvA0uBPdWHb6Kqlon3J6me6phLV9DIe5NJIEmSJEmSpBbA7WCSJEmSJEktgEkgSZIkSZKkFsAkkCRJkiRJUgtgEkiSJEmSJKkFMAkkSZIkSZLUApgEkiRJkiRJagFMAkmSJEmSJLUAJoEkSZIkSZJagP8PmGBwczHP4iEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g99H43u-PO1N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}